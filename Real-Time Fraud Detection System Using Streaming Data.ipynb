{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf64303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyspark\n",
      "Failed to build pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "    Running setup.py install for pyspark: started\n",
      "    Running setup.py install for pyspark: finished with status 'done'\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\HP\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\pip-install-j1a5zo66\\\\pyspark_ce0fa65ce7634baaa13e68df4535dc57\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\pip-install-j1a5zo66\\\\pyspark_ce0fa65ce7634baaa13e68df4535dc57\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\HP\\AppData\\Local\\Temp\\pip-wheel-ckabd2gd'\n",
      "       cwd: C:\\Users\\HP\\AppData\\Local\\Temp\\pip-install-j1a5zo66\\pyspark_ce0fa65ce7634baaa13e68df4535dc57\\\n",
      "  Complete output (2554 lines):\n",
      "  C:\\Users\\HP\\anaconda3\\lib\\site-packages\\setuptools\\dist.py:757: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib\n",
      "  creating build\\lib\\pyspark\n",
      "  copying pyspark\\accumulators.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\broadcast.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\conf.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\context.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\daemon.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\files.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\find_spark_home.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\install.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\instrumentation_utils.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\java_gateway.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\join.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\profiler.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\rdd.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\rddsampler.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\resultiterable.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\serializers.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\shell.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\shuffle.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\statcounter.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\status.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\storagelevel.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\taskcontext.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\traceback_utils.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\util.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\version.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\worker.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\worker_util.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\_globals.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\__init__.py -> build\\lib\\pyspark\n",
      "  creating build\\lib\\pyspark\\cloudpickle\n",
      "  copying pyspark\\cloudpickle\\cloudpickle.py -> build\\lib\\pyspark\\cloudpickle\n",
      "  copying pyspark\\cloudpickle\\cloudpickle_fast.py -> build\\lib\\pyspark\\cloudpickle\n",
      "  copying pyspark\\cloudpickle\\compat.py -> build\\lib\\pyspark\\cloudpickle\n",
      "  copying pyspark\\cloudpickle\\__init__.py -> build\\lib\\pyspark\\cloudpickle\n",
      "  creating build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\classification.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\clustering.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\common.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\evaluation.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\feature.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\fpm.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\random.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\recommendation.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\regression.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\tree.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\util.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\__init__.py -> build\\lib\\pyspark\\mllib\n",
      "  creating build\\lib\\pyspark\\ml\n",
      "  creating build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\base.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\classification.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\evaluation.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\feature.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\functions.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\io_utils.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\pipeline.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\summarizer.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\tuning.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\util.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\__init__.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  creating build\\lib\\pyspark\\mllib\\linalg\n",
      "  copying pyspark\\mllib\\linalg\\distributed.py -> build\\lib\\pyspark\\mllib\\linalg\n",
      "  copying pyspark\\mllib\\linalg\\__init__.py -> build\\lib\\pyspark\\mllib\\linalg\n",
      "  creating build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\distribution.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\KernelDensity.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\test.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\_statistics.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\__init__.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\ml\\base.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\classification.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\clustering.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\common.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\dl_util.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\evaluation.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\feature.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\fpm.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\functions.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\image.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\model_cache.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\pipeline.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\recommendation.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\regression.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\stat.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\tree.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\tuning.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\util.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\wrapper.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\__init__.py -> build\\lib\\pyspark\\ml\n",
      "  creating build\\lib\\pyspark\\ml\\linalg\n",
      "  copying pyspark\\ml\\linalg\\__init__.py -> build\\lib\\pyspark\\ml\\linalg\n",
      "  creating build\\lib\\pyspark\\ml\\param\n",
      "  copying pyspark\\ml\\param\\shared.py -> build\\lib\\pyspark\\ml\\param\n",
      "  copying pyspark\\ml\\param\\_shared_params_code_gen.py -> build\\lib\\pyspark\\ml\\param\n",
      "  copying pyspark\\ml\\param\\__init__.py -> build\\lib\\pyspark\\ml\\param\n",
      "  creating build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\data.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\distributor.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\log_communication.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\torch_run_process_wrapper.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\__init__.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  creating build\\lib\\pyspark\\ml\\deepspeed\n",
      "  copying pyspark\\ml\\deepspeed\\deepspeed_distributor.py -> build\\lib\\pyspark\\ml\\deepspeed\n",
      "  copying pyspark\\ml\\deepspeed\\__init__.py -> build\\lib\\pyspark\\ml\\deepspeed\n",
      "  creating build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\catalog.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\column.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\conf.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\context.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\dataframe.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\functions.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\group.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\observation.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\readwriter.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\session.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\sql_formatter.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\types.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\udf.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\udtf.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\utils.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\window.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\__init__.py -> build\\lib\\pyspark\\sql\n",
      "  creating build\\lib\\pyspark\\sql\\avro\n",
      "  copying pyspark\\sql\\avro\\functions.py -> build\\lib\\pyspark\\sql\\avro\n",
      "  copying pyspark\\sql\\avro\\__init__.py -> build\\lib\\pyspark\\sql\\avro\n",
      "  creating build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\catalog.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\column.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\conf.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\conversion.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\dataframe.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\expressions.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\functions.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\group.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\plan.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\readwriter.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\session.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\types.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\udf.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\udtf.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\utils.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\window.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\_typing.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\__init__.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\avro\n",
      "  copying pyspark\\sql\\connect\\avro\\functions.py -> build\\lib\\pyspark\\sql\\connect\\avro\n",
      "  copying pyspark\\sql\\connect\\avro\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\avro\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\artifact.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\core.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\reattach.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\base_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\base_pb2_grpc.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\catalog_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\commands_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\common_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\example_plugins_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\expressions_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\relations_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\types_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\protobuf\n",
      "  copying pyspark\\sql\\connect\\protobuf\\functions.py -> build\\lib\\pyspark\\sql\\connect\\protobuf\n",
      "  copying pyspark\\sql\\connect\\protobuf\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\protobuf\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  copying pyspark\\sql\\connect\\streaming\\query.py -> build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  copying pyspark\\sql\\connect\\streaming\\readwriter.py -> build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  copying pyspark\\sql\\connect\\streaming\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  creating build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\conversion.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\functions.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\group_ops.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\map_ops.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\serializers.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\typehints.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\types.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\utils.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\__init__.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  creating build\\lib\\pyspark\\sql\\protobuf\n",
      "  copying pyspark\\sql\\protobuf\\functions.py -> build\\lib\\pyspark\\sql\\protobuf\n",
      "  copying pyspark\\sql\\protobuf\\__init__.py -> build\\lib\\pyspark\\sql\\protobuf\n",
      "  creating build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\listener.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\query.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\readwriter.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\state.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\__init__.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  creating build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\context.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\dstream.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\kinesis.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\listener.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\util.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\__init__.py -> build\\lib\\pyspark\\streaming\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying pyspark\\sql\\connect\\streaming\\worker\\foreach_batch_worker.py -> build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying pyspark\\sql\\connect\\streaming\\worker\\listener_worker.py -> build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying pyspark\\sql\\connect\\streaming\\worker\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  package init file 'deps\\bin\\__init__.py' not found (or not a regular file)\n",
      "  package init file 'deps\\sbin\\__init__.py' not found (or not a regular file)\n",
      "  package init file 'deps\\jars\\__init__.py' not found (or not a regular file)\n",
      "  creating build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\accessors.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\base.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\categorical.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\config.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\correlation.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\datetimes.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\exceptions.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\extensions.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\frame.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\generic.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\groupby.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\indexing.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\internal.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\mlflow.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\namespace.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\numpy_compat.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\resample.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\series.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\sql_formatter.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\sql_processor.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\strings.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\supported_api_gen.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\utils.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\window.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\_typing.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\__init__.py -> build\\lib\\pyspark\\pandas\n",
      "  creating build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\base.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\binary_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\boolean_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\categorical_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\complex_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\datetime_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\date_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\null_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\num_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\string_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\timedelta_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\udt_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\__init__.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  creating build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\base.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\category.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\datetimes.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\multi.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\numeric.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\timedelta.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\__init__.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  creating build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\common.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\frame.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\general_functions.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\groupby.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\indexes.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\resample.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\scalars.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\series.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\window.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\__init__.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  creating build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\core.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\matplotlib.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\plotly.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\__init__.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  creating build\\lib\\pyspark\\pandas\\spark\n",
      "  copying pyspark\\pandas\\spark\\accessors.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "  copying pyspark\\pandas\\spark\\functions.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "  copying pyspark\\pandas\\spark\\utils.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "  copying pyspark\\pandas\\spark\\__init__.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "  creating build\\lib\\pyspark\\pandas\\typedef\n",
      "  copying pyspark\\pandas\\typedef\\typehints.py -> build\\lib\\pyspark\\pandas\\typedef\n",
      "  copying pyspark\\pandas\\typedef\\__init__.py -> build\\lib\\pyspark\\pandas\\typedef\n",
      "  creating build\\lib\\pyspark\\pandas\\usage_logging\n",
      "  copying pyspark\\pandas\\usage_logging\\usage_logger.py -> build\\lib\\pyspark\\pandas\\usage_logging\n",
      "  copying pyspark\\pandas\\usage_logging\\__init__.py -> build\\lib\\pyspark\\pandas\\usage_logging\n",
      "  package init file 'pyspark\\python\\pyspark\\__init__.py' not found (or not a regular file)\n",
      "  creating build\\lib\\pyspark\\python\n",
      "  creating build\\lib\\pyspark\\python\\pyspark\n",
      "  copying pyspark\\python\\pyspark\\shell.py -> build\\lib\\pyspark\\python\\pyspark\n",
      "  package init file 'lib\\__init__.py' not found (or not a regular file)\n",
      "  creating build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\connectutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\mllibutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\mlutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\pandasutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\sqlutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\streamingutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\utils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\__init__.py -> build\\lib\\pyspark\\testing\n",
      "  package init file 'deps\\data\\__init__.py' not found (or not a regular file)\n",
      "  package init file 'deps\\licenses\\__init__.py' not found (or not a regular file)\n",
      "  creating build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\information.py -> build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\profile.py -> build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\requests.py -> build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\__init__.py -> build\\lib\\pyspark\\resource\n",
      "  creating build\\lib\\pyspark\\errors\n",
      "  copying pyspark\\errors\\error_classes.py -> build\\lib\\pyspark\\errors\n",
      "  copying pyspark\\errors\\utils.py -> build\\lib\\pyspark\\errors\n",
      "  copying pyspark\\errors\\__init__.py -> build\\lib\\pyspark\\errors\n",
      "  creating build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\base.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\captured.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\connect.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\__init__.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  creating build\\lib\\pyspark\\examples\n",
      "  creating build\\lib\\pyspark\\examples\\src\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\als.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\avro_inputformat.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\kmeans.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\logistic_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\pagerank.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\parquet_inputformat.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\pi.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\sort.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\status_api_demo.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\transitive_closure.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  running egg_info\n",
      "  writing pyspark.egg-info\\PKG-INFO\n",
      "  writing dependency_links to pyspark.egg-info\\dependency_links.txt\n",
      "  writing requirements to pyspark.egg-info\\requires.txt\n",
      "  writing top-level names to pyspark.egg-info\\top_level.txt\n",
      "  reading manifest file 'pyspark.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.py[cod]' found anywhere in distribution\n",
      "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
      "  writing manifest file 'pyspark.egg-info\\SOURCES.txt'\n",
      "  copying pyspark\\_typing.pyi -> build\\lib\\pyspark\n",
      "  copying pyspark\\py.typed -> build\\lib\\pyspark\n",
      "  copying pyspark\\mllib\\_typing.pyi -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\ml\\_typing.pyi -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\sql\\_typing.pyi -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\connect\\proto\\base_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\catalog_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\commands_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\common_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\example_plugins_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\expressions_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\relations_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\types_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\pandas\\functions.pyi -> build\\lib\\pyspark\\sql\\pandas\n",
      "  creating build\\lib\\pyspark\\sql\\pandas\\_typing\n",
      "  copying pyspark\\sql\\pandas\\_typing\\__init__.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\n",
      "  creating build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying pyspark\\sql\\pandas\\_typing\\protocols\\__init__.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying pyspark\\sql\\pandas\\_typing\\protocols\\frame.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying pyspark\\sql\\pandas\\_typing\\protocols\\series.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  creating build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\beeline -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\beeline.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\docker-image-tool.sh -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\find-spark-home -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\find-spark-home.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\load-spark-env.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\load-spark-env.sh -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\pyspark -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\pyspark.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\pyspark2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\run-example -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\run-example.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-class -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-class.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-class2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-connect-shell -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-shell -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-shell.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-shell2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-sql -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-sql.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-sql2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-submit -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-submit.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-submit2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\sparkR -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\sparkR.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\sparkR2.cmd -> build\\lib\\pyspark\\bin\n",
      "  creating build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\spark-config.sh -> build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\spark-daemon.sh -> build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\start-history-server.sh -> build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\stop-history-server.sh -> build\\lib\\pyspark\\sbin\n",
      "  creating build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\HikariCP-2.5.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\JLargeArrays-1.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\JTransforms-3.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\RoaringBitmap-0.9.45.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\ST4-4.0.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\activation-1.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\aircompressor-0.27.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\algebra_2.12-2.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\annotations-17.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\antlr-runtime-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\antlr4-runtime-4.9.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\aopalliance-repackaged-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arpack-3.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arpack_combined_all-0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-format-12.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-memory-core-12.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-memory-netty-12.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-vector-12.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\audience-annotations-0.5.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\avro-1.11.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\avro-ipc-1.11.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\avro-mapred-1.11.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\blas-3.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\bonecp-0.8.0.RELEASE.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\breeze-macros_2.12-2.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\breeze_2.12-2.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\cats-kernel_2.12-2.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\chill-java-0.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\chill_2.12-0.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-cli-1.5.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-codec-1.16.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-collections-3.2.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-collections4-4.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-compiler-3.1.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-compress-1.23.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-crypto-1.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-dbcp-1.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-io-2.16.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-lang-2.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-lang3-3.12.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-logging-1.1.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-math3-3.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-pool-1.5.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-text-1.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\compress-lzf-1.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\curator-client-2.13.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\curator-framework-2.13.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\curator-recipes-2.13.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datanucleus-api-jdo-4.2.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datanucleus-core-4.1.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datanucleus-rdbms-4.1.19.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datasketches-java-3.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datasketches-memory-2.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\derby-10.14.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\flatbuffers-java-1.12.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\gson-2.2.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\guava-14.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hadoop-client-api-3.3.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hadoop-client-runtime-3.3.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hadoop-shaded-guava-1.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hadoop-yarn-server-web-proxy-3.3.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-beeline-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-cli-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-common-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-exec-2.3.9-core.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-jdbc-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-llap-common-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-metastore-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-serde-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-service-rpc-3.1.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-0.23-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-common-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-scheduler-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-storage-api-2.8.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hk2-api-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hk2-locator-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hk2-utils-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\httpclient-4.5.14.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\httpcore-4.4.16.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\istack-commons-runtime-3.0.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\ivy-2.5.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-annotations-2.15.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-core-2.15.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-core-asl-1.9.13.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-databind-2.15.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-dataformat-yaml-2.15.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-datatype-jsr310-2.15.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-mapper-asl-1.9.13.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-module-scala_2.12-2.15.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.annotation-api-1.3.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.inject-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.servlet-api-4.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.validation-api-2.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.ws.rs-api-2.1.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.xml.bind-api-2.3.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\janino-3.1.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\javassist-3.29.2-GA.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\javax.jdo-3.2.0-m3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\javolution-5.5.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jaxb-runtime-2.3.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jcl-over-slf4j-2.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jdo-api-3.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-client-2.40.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-common-2.40.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-container-servlet-2.40.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-container-servlet-core-2.40.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-hk2-2.40.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-server-2.40.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jline-2.14.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\joda-time-2.12.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jodd-core-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jpam-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json-1.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-ast_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-core_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-jackson_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-scalap_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jsr305-3.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jta-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jul-to-slf4j-2.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kryo-shaded-4.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-client-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-client-api-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-httpclient-okhttp-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-admissionregistration-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-apiextensions-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-apps-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-autoscaling-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-batch-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-certificates-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-common-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-coordination-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-core-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-discovery-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-events-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-extensions-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-flowcontrol-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-gatewayapi-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-metrics-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-networking-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-node-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-policy-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-rbac-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-resource-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-scheduling-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-storageclass-6.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\lapack-3.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\leveldbjni-all-1.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\libfb303-0.9.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\libthrift-0.12.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-1.2-api-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-api-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-core-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-slf4j2-impl-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\logging-interceptor-3.12.12.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\lz4-java-1.8.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\mesos-1.4.3-shaded-protobuf.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-core-4.2.19.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-graphite-4.2.19.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-jmx-4.2.19.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-json-4.2.19.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-jvm-4.2.19.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\minlog-1.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-all-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-buffer-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-http-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-http2-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-socks-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-common-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-handler-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-handler-proxy-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-resolver-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-classes-epoll-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-classes-kqueue-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-unix-common-4.1.96.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\objenesis-3.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\okhttp-3.12.12.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\okio-1.15.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\opencsv-2.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\orc-core-1.9.4-shaded-protobuf.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\orc-mapreduce-1.9.4-shaded-protobuf.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\orc-shims-1.9.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\oro-2.0.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\osgi-resource-locator-1.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\paranamer-2.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-column-1.13.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-common-1.13.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-encoding-1.13.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-format-structures-1.13.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-hadoop-1.13.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-jackson-1.13.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\pickle-1.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\py4j-0.10.9.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\rocksdbjni-8.3.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-collection-compat_2.12-2.7.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-compiler-2.12.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-library-2.12.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-parser-combinators_2.12-2.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-reflect-2.12.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-xml_2.12-2.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\shims-0.9.45.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\slf4j-api-2.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\snakeyaml-2.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\snakeyaml-engine-2.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\snappy-java-1.1.10.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-catalyst_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-common-utils_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-core_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-graphx_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-hive-thriftserver_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-hive_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-kubernetes_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-kvstore_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-launcher_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-mesos_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-mllib-local_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-mllib_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-network-common_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-network-shuffle_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-repl_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-sketch_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-sql-api_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-sql_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-streaming_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-tags_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-unsafe_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-yarn_2.12-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire-macros_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire-platform_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire-util_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\stax-api-1.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\stream-2.9.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\super-csv-2.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\threeten-extra-1.7.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\tink-1.9.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\transaction-api-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\univocity-parsers-2.9.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\xbean-asm9-shaded-4.23.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\xz-1.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zjsonpatch-0.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zookeeper-3.6.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zookeeper-jute-3.6.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zstd-jni-1.5.5-4.jar -> build\\lib\\pyspark\\jars\n",
      "  creating build\\lib\\pyspark\\python\\lib\n",
      "  copying lib\\py4j-0.10.9.7-src.zip -> build\\lib\\pyspark\\python\\lib\n",
      "  copying lib\\pyspark.zip -> build\\lib\\pyspark\\python\\lib\n",
      "  creating build\\lib\\pyspark\\data\n",
      "  creating build\\lib\\pyspark\\data\\artifact-tests\n",
      "  creating build\\lib\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying deps\\data\\artifact-tests\\crc\\junitLargeJar.txt -> build\\lib\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying deps\\data\\artifact-tests\\crc\\smallJar.txt -> build\\lib\\pyspark\\data\\artifact-tests\\crc\n",
      "  creating build\\lib\\pyspark\\data\\graphx\n",
      "  copying deps\\data\\graphx\\followers.txt -> build\\lib\\pyspark\\data\\graphx\n",
      "  copying deps\\data\\graphx\\users.txt -> build\\lib\\pyspark\\data\\graphx\n",
      "  creating build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\gmm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\kmeans_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\pagerank_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\pic_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_binary_classification_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_fpgrowth.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_isotonic_regression_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_kmeans_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_lda_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_lda_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_linear_regression_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_movielens_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_multiclass_classification_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_svm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\streaming_kmeans_data_test.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\als\n",
      "  copying deps\\data\\mllib\\als\\sample_movielens_ratings.txt -> build\\lib\\pyspark\\data\\mllib\\als\n",
      "  copying deps\\data\\mllib\\als\\test.data -> build\\lib\\pyspark\\data\\mllib\\als\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\images\n",
      "  copying deps\\data\\mllib\\images\\license.txt -> build\\lib\\pyspark\\data\\mllib\\images\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\images\\origin\n",
      "  copying deps\\data\\mllib\\images\\origin\\license.txt -> build\\lib\\pyspark\\data\\mllib\\images\\origin\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  copying deps\\data\\mllib\\images\\origin\\kittens\\not-image.txt -> build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\ridge-data\n",
      "  copying deps\\data\\mllib\\ridge-data\\lpsa.data -> build\\lib\\pyspark\\data\\mllib\\ridge-data\n",
      "  creating build\\lib\\pyspark\\data\\streaming\n",
      "  copying deps\\data\\streaming\\AFINN-111.txt -> build\\lib\\pyspark\\data\\streaming\n",
      "  creating build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-AnchorJS.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-CC0.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-bootstrap.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-cloudpickle.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-d3.min.js.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-dagre-d3.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-datatables.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-graphlib-dot.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-jdom.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-join.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-jquery.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-json-formatter.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-matchMedia-polyfill.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-modernizr.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-mustache.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-py4j.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-respond.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-sbt-launch-lib.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-sorttable.js.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-vis-timeline.txt -> build\\lib\\pyspark\\licenses\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\aft_survival_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\als_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\binarizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\bisecting_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\bucketed_random_projection_lsh_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\bucketizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\chi_square_test_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\chisq_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\correlation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\count_vectorizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\cross_validator.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\dataframe_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\dct_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\decision_tree_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\decision_tree_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\elementwise_product_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\estimator_transformer_param_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\feature_hasher_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\fm_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\fm_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\fpgrowth_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\gaussian_mixture_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\generalized_linear_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\gradient_boosted_tree_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\gradient_boosted_tree_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\imputer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\index_to_string_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\interaction_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\isotonic_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\kmeans_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\lda_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\linear_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\linearsvc.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\logistic_regression_summary_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\logistic_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\max_abs_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\min_hash_lsh_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\min_max_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\multiclass_logistic_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\multilayer_perceptron_classification.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\n_gram_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\naive_bayes_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\normalizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\one_vs_rest_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\onehot_encoder_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\pca_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\pipeline_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\polynomial_expansion_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\power_iteration_clustering_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\prefixspan_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\quantile_discretizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\random_forest_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\random_forest_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\rformula_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\robust_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\sql_transformer.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\standard_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\stopwords_remover_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\string_indexer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\summarizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\tf_idf_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\tokenizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\train_validation_split.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\univariate_feature_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\variance_threshold_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_assembler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_indexer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_size_hint_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_slicer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\word2vec_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\binary_classification_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\bisecting_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\correlations.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\correlations_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\decision_tree_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\decision_tree_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\elementwise_product_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\fpgrowth_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gaussian_mixture_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gaussian_mixture_model.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gradient_boosting_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gradient_boosting_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\hypothesis_testing_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\hypothesis_testing_kolmogorov_smirnov_test_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\isotonic_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\kernel_density_estimation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\kmeans.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\latent_dirichlet_allocation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\linear_regression_with_sgd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\logistic_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\logistic_regression_with_lbfgs_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\multi_class_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\multi_label_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\naive_bayes_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\normalizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\pca_rowmatrix_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\power_iteration_clustering_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\random_forest_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\random_forest_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\random_rdd_generation.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\ranking_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\recommendation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\regression_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\sampled_rdds.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\standard_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\stratified_sampling_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\streaming_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\streaming_linear_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\summary_statistics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\svd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\svm_with_sgd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\tf_idf_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\word2vec.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\word2vec_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\arrow.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\basic.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\datasource.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\hive.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\udtf.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_kafka_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_network_wordcount_session_window.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_network_wordcount_windowed.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_sessionization.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\hdfs_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\network_wordjoinsentiments.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\queue_stream.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\recoverable_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\sql_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\stateful_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  running build_scripts\n",
      "  creating build\\scripts-3.9\n",
      "  copying deps\\bin\\beeline -> build\\scripts-3.9\n",
      "  copying deps\\bin\\beeline.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\docker-image-tool.sh -> build\\scripts-3.9\n",
      "  copying deps\\bin\\find-spark-home -> build\\scripts-3.9\n",
      "  copying deps\\bin\\find-spark-home.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\load-spark-env.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\load-spark-env.sh -> build\\scripts-3.9\n",
      "  copying deps\\bin\\pyspark -> build\\scripts-3.9\n",
      "  copying deps\\bin\\pyspark.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\pyspark2.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\run-example -> build\\scripts-3.9\n",
      "  copying deps\\bin\\run-example.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-class -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-class.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-class2.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-connect-shell -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-shell -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-shell.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-shell2.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-sql -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-sql.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-sql2.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-submit -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-submit.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\spark-submit2.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\sparkR -> build\\scripts-3.9\n",
      "  copying deps\\bin\\sparkR.cmd -> build\\scripts-3.9\n",
      "  copying deps\\bin\\sparkR2.cmd -> build\\scripts-3.9\n",
      "  copying and adjusting pyspark\\find_spark_home.py -> build\\scripts-3.9\n",
      "  C:\\Users\\HP\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  installing to build\\bdist.win-amd64\\wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build\\bdist.win-amd64\n",
      "  creating build\\bdist.win-amd64\\wheel\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\n",
      "  copying build\\lib\\pyspark\\accumulators.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\beeline -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\beeline.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\docker-image-tool.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\find-spark-home -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\find-spark-home.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\load-spark-env.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\load-spark-env.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\pyspark -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\pyspark.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\pyspark2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\run-example -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\run-example.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-class -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-class.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-class2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-connect-shell -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-shell -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-shell.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-shell2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-sql -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-sql.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-sql2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-submit -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-submit.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-submit2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\sparkR -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\sparkR.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\sparkR2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\broadcast.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\cloudpickle\\cloudpickle.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\cloudpickle\\cloudpickle_fast.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\cloudpickle\\compat.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\cloudpickle\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\conf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\context.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\daemon.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\artifact-tests\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying build\\lib\\pyspark\\data\\artifact-tests\\crc\\junitLargeJar.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying build\\lib\\pyspark\\data\\artifact-tests\\crc\\smallJar.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\artifact-tests\\crc\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\graphx\n",
      "  copying build\\lib\\pyspark\\data\\graphx\\followers.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\graphx\n",
      "  copying build\\lib\\pyspark\\data\\graphx\\users.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\graphx\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\als\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\als\\sample_movielens_ratings.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\als\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\als\\test.data -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\als\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\gmm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\images\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\images\\license.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\images\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\images\\origin\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\\not-image.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\images\\origin\\license.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\images\\origin\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\kmeans_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\pagerank_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\pic_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\ridge-data\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\ridge-data\\lpsa.data -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\ridge-data\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_binary_classification_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_fpgrowth.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_isotonic_regression_libsvm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_kmeans_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_lda_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_lda_libsvm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_libsvm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_linear_regression_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_movielens_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_multiclass_classification_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_svm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\streaming_kmeans_data_test.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\streaming\n",
      "  copying build\\lib\\pyspark\\data\\streaming\\AFINN-111.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\streaming\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\errors\n",
      "  copying build\\lib\\pyspark\\errors\\error_classes.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\base.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\captured.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\connect.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\n",
      "  copying build\\lib\\pyspark\\errors\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\als.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\avro_inputformat.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\kmeans.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\logistic_regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\aft_survival_regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\als_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\binarizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\bisecting_k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\bucketed_random_projection_lsh_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\bucketizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\chisq_selector_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\chi_square_test_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\correlation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\count_vectorizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\cross_validator.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\dataframe_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\dct_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\decision_tree_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\decision_tree_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\elementwise_product_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\estimator_transformer_param_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\feature_hasher_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\fm_classifier_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\fm_regressor_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\fpgrowth_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\gaussian_mixture_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\generalized_linear_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\gradient_boosted_tree_classifier_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\gradient_boosted_tree_regressor_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\imputer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\index_to_string_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\interaction_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\isotonic_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\kmeans_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\lda_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\linearsvc.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\linear_regression_with_elastic_net.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\logistic_regression_summary_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\logistic_regression_with_elastic_net.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\max_abs_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\min_hash_lsh_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\min_max_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\multiclass_logistic_regression_with_elastic_net.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\multilayer_perceptron_classification.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\naive_bayes_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\normalizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\n_gram_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\onehot_encoder_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\one_vs_rest_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\pca_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\pipeline_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\polynomial_expansion_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\power_iteration_clustering_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\prefixspan_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\quantile_discretizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\random_forest_classifier_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\random_forest_regressor_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\rformula_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\robust_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\sql_transformer.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\standard_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\stopwords_remover_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\string_indexer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\summarizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\tf_idf_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\tokenizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\train_validation_split.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\univariate_feature_selector_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\variance_threshold_selector_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_assembler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_indexer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_size_hint_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_slicer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\word2vec_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\binary_classification_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\bisecting_k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\correlations.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\correlations_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\decision_tree_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\decision_tree_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\elementwise_product_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\fpgrowth_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gaussian_mixture_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gaussian_mixture_model.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gradient_boosting_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gradient_boosting_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\hypothesis_testing_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\hypothesis_testing_kolmogorov_smirnov_test_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\isotonic_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\kernel_density_estimation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\kmeans.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\latent_dirichlet_allocation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\linear_regression_with_sgd_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\logistic_regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\logistic_regression_with_lbfgs_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\multi_class_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\multi_label_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\naive_bayes_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\normalizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\pca_rowmatrix_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\power_iteration_clustering_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\random_forest_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\random_forest_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\random_rdd_generation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\ranking_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\recommendation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\regression_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\sampled_rdds.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\standard_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\stratified_sampling_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\streaming_k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\streaming_linear_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\summary_statistics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\svd_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\svm_with_sgd_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\tf_idf_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\word2vec.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\word2vec_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\pagerank.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\parquet_inputformat.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\pi.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sort.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\arrow.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\basic.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\datasource.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\hive.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_kafka_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_network_wordcount_session_window.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_network_wordcount_windowed.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_sessionization.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\udtf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\status_api_demo.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\hdfs_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\network_wordjoinsentiments.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\queue_stream.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\recoverable_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\sql_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\stateful_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\transitive_closure.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\files.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\find_spark_home.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\install.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\instrumentation_utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\activation-1.1.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\aircompressor-0.27.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\algebra_2.12-2.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\annotations-17.0.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\antlr-runtime-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\antlr4-runtime-4.9.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\aopalliance-repackaged-2.6.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arpack-3.0.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arpack_combined_all-0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-format-12.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-memory-core-12.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-memory-netty-12.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-vector-12.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\audience-annotations-0.5.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\avro-1.11.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\avro-ipc-1.11.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\avro-mapred-1.11.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\blas-3.0.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\bonecp-0.8.0.RELEASE.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\breeze-macros_2.12-2.1.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\breeze_2.12-2.1.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\cats-kernel_2.12-2.1.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\chill-java-0.10.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\chill_2.12-0.10.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-cli-1.5.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-codec-1.16.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-collections-3.2.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-collections4-4.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-compiler-3.1.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-compress-1.23.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-crypto-1.1.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-dbcp-1.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-io-2.16.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-lang-2.6.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-lang3-3.12.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-logging-1.1.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-math3-3.6.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-pool-1.5.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\commons-text-1.10.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\compress-lzf-1.1.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\curator-client-2.13.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\curator-framework-2.13.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\curator-recipes-2.13.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\datanucleus-api-jdo-4.2.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\datanucleus-core-4.1.17.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\datanucleus-rdbms-4.1.19.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\datasketches-java-3.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\datasketches-memory-2.1.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\derby-10.14.2.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\flatbuffers-java-1.12.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\gson-2.2.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\guava-14.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hadoop-client-api-3.3.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hadoop-client-runtime-3.3.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hadoop-shaded-guava-1.1.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hadoop-yarn-server-web-proxy-3.3.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\HikariCP-2.5.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-beeline-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-cli-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-common-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-exec-2.3.9-core.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-jdbc-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-llap-common-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-metastore-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-serde-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-service-rpc-3.1.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-shims-0.23-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-shims-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-shims-common-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-shims-scheduler-2.3.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hive-storage-api-2.8.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hk2-api-2.6.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hk2-locator-2.6.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\hk2-utils-2.6.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\httpclient-4.5.14.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\httpcore-4.4.16.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\istack-commons-runtime-3.0.8.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\ivy-2.5.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-annotations-2.15.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-core-2.15.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-core-asl-1.9.13.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-databind-2.15.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-dataformat-yaml-2.15.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-datatype-jsr310-2.15.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-mapper-asl-1.9.13.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jackson-module-scala_2.12-2.15.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jakarta.annotation-api-1.3.5.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jakarta.inject-2.6.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jakarta.servlet-api-4.0.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jakarta.validation-api-2.0.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jakarta.ws.rs-api-2.1.6.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jakarta.xml.bind-api-2.3.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\janino-3.1.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\javassist-3.29.2-GA.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\javax.jdo-3.2.0-m3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\javolution-5.5.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jaxb-runtime-2.3.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jcl-over-slf4j-2.0.7.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jdo-api-3.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jersey-client-2.40.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jersey-common-2.40.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jersey-container-servlet-2.40.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jersey-container-servlet-core-2.40.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jersey-hk2-2.40.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jersey-server-2.40.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\JLargeArrays-1.5.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jline-2.14.6.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\joda-time-2.12.5.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jodd-core-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jpam-1.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\json-1.8.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\json4s-ast_2.12-3.7.0-M11.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\json4s-core_2.12-3.7.0-M11.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\json4s-jackson_2.12-3.7.0-M11.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\json4s-scalap_2.12-3.7.0-M11.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jsr305-3.0.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jta-1.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\JTransforms-3.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\jul-to-slf4j-2.0.7.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kryo-shaded-4.0.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-client-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-client-api-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-httpclient-okhttp-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-admissionregistration-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-apiextensions-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-apps-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-autoscaling-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-batch-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-certificates-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-common-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-coordination-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-core-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-discovery-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-events-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-extensions-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-flowcontrol-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-gatewayapi-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-metrics-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-networking-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-node-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-policy-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-rbac-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-resource-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-scheduling-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\kubernetes-model-storageclass-6.7.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\lapack-3.0.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\leveldbjni-all-1.8.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\libfb303-0.9.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\libthrift-0.12.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\log4j-1.2-api-2.20.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\log4j-api-2.20.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\log4j-core-2.20.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\log4j-slf4j2-impl-2.20.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\logging-interceptor-3.12.12.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\lz4-java-1.8.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\mesos-1.4.3-shaded-protobuf.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\metrics-core-4.2.19.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\metrics-graphite-4.2.19.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\metrics-jmx-4.2.19.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\metrics-json-4.2.19.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\metrics-jvm-4.2.19.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\minlog-1.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-all-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-buffer-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-codec-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-codec-http-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-codec-http2-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-codec-socks-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-common-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-handler-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-handler-proxy-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-resolver-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-classes-epoll-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-classes-kqueue-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\netty-transport-native-unix-common-4.1.96.Final.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\objenesis-3.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\okhttp-3.12.12.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\okio-1.15.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\opencsv-2.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\orc-core-1.9.4-shaded-protobuf.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\orc-mapreduce-1.9.4-shaded-protobuf.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\orc-shims-1.9.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\oro-2.0.8.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\osgi-resource-locator-1.0.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\paranamer-2.8.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\parquet-column-1.13.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\parquet-common-1.13.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\parquet-encoding-1.13.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\parquet-format-structures-1.13.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\parquet-hadoop-1.13.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\parquet-jackson-1.13.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\pickle-1.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\py4j-0.10.9.7.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\RoaringBitmap-0.9.45.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\rocksdbjni-8.3.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\scala-collection-compat_2.12-2.7.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\scala-compiler-2.12.18.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\scala-library-2.12.18.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\scala-parser-combinators_2.12-2.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\scala-reflect-2.12.18.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\scala-xml_2.12-2.1.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\shims-0.9.45.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\slf4j-api-2.0.7.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\snakeyaml-2.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\snakeyaml-engine-2.6.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\snappy-java-1.1.10.5.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-catalyst_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-common-utils_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-core_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-graphx_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-hive-thriftserver_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-hive_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-kubernetes_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-kvstore_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-launcher_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-mesos_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-mllib-local_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-mllib_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-network-common_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-network-shuffle_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-repl_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-sketch_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-sql-api_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-sql_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-streaming_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-tags_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-unsafe_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spark-yarn_2.12-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spire-macros_2.12-0.17.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spire-platform_2.12-0.17.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spire-util_2.12-0.17.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\spire_2.12-0.17.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\ST4-4.0.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\stax-api-1.0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\stream-2.9.6.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\super-csv-2.2.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\threeten-extra-1.7.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\tink-1.9.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\transaction-api-1.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\univocity-parsers-2.9.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\xbean-asm9-shaded-4.23.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\xz-1.9.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\zjsonpatch-0.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\zookeeper-3.6.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\zookeeper-jute-3.6.3.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\zstd-jni-1.5.5-4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\java_gateway.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\join.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-AnchorJS.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-bootstrap.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-CC0.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-cloudpickle.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-d3.min.js.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-dagre-d3.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-datatables.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-graphlib-dot.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-jdom.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-join.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-jquery.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-json-formatter.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-matchMedia-polyfill.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-modernizr.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-mustache.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-py4j.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-respond.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-sbt-launch-lib.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-sorttable.js.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  copying build\\lib\\pyspark\\licenses\\LICENSE-vis-timeline.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\licenses\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\base.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\classification.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\clustering.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\common.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\base.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\classification.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\evaluation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\feature.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\io_utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\pipeline.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\summarizer.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\tuning.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\util.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  copying build\\lib\\pyspark\\ml\\connect\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\connect\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\ml\\deepspeed\n",
      "  copying build\\lib\\pyspark\\ml\\deepspeed\\deepspeed_distributor.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\deepspeed\n",
      "  copying build\\lib\\pyspark\\ml\\deepspeed\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\deepspeed\n",
      "  copying build\\lib\\pyspark\\ml\\dl_util.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\evaluation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\feature.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\fpm.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\image.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\ml\\linalg\n",
      "  copying build\\lib\\pyspark\\ml\\linalg\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\linalg\n",
      "  copying build\\lib\\pyspark\\ml\\model_cache.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\ml\\param\n",
      "  copying build\\lib\\pyspark\\ml\\param\\shared.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\param\n",
      "  copying build\\lib\\pyspark\\ml\\param\\_shared_params_code_gen.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\param\n",
      "  copying build\\lib\\pyspark\\ml\\param\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\param\n",
      "  copying build\\lib\\pyspark\\ml\\pipeline.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\recommendation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\stat.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\ml\\torch\n",
      "  copying build\\lib\\pyspark\\ml\\torch\\data.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\torch\n",
      "  copying build\\lib\\pyspark\\ml\\torch\\distributor.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\torch\n",
      "  copying build\\lib\\pyspark\\ml\\torch\\log_communication.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\torch\n",
      "  copying build\\lib\\pyspark\\ml\\torch\\torch_run_process_wrapper.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\torch\n",
      "  copying build\\lib\\pyspark\\ml\\torch\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\\torch\n",
      "  copying build\\lib\\pyspark\\ml\\tree.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\tuning.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\util.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\wrapper.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\_typing.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  copying build\\lib\\pyspark\\ml\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\ml\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\classification.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\clustering.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\common.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\evaluation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\feature.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\fpm.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\mllib\\linalg\n",
      "  copying build\\lib\\pyspark\\mllib\\linalg\\distributed.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\\linalg\n",
      "  copying build\\lib\\pyspark\\mllib\\linalg\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\\linalg\n",
      "  copying build\\lib\\pyspark\\mllib\\random.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\recommendation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\mllib\\stat\n",
      "  copying build\\lib\\pyspark\\mllib\\stat\\distribution.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\\stat\n",
      "  copying build\\lib\\pyspark\\mllib\\stat\\KernelDensity.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\\stat\n",
      "  copying build\\lib\\pyspark\\mllib\\stat\\test.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\\stat\n",
      "  copying build\\lib\\pyspark\\mllib\\stat\\_statistics.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\\stat\n",
      "  copying build\\lib\\pyspark\\mllib\\stat\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\\stat\n",
      "  copying build\\lib\\pyspark\\mllib\\tree.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\util.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\_typing.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  copying build\\lib\\pyspark\\mllib\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\accessors.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\base.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\categorical.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\config.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\correlation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\base.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\binary_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\boolean_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\categorical_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\complex_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\datetime_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\date_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\null_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\num_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\string_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\timedelta_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\udt_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\data_type_ops\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\data_type_ops\n",
      "  copying build\\lib\\pyspark\\pandas\\datetimes.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\exceptions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\extensions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\frame.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\generic.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\groupby.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexes\\base.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexes\\category.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexes\\datetimes.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexes\\multi.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexes\\numeric.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexes\\timedelta.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexes\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\indexes\n",
      "  copying build\\lib\\pyspark\\pandas\\indexing.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\internal.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\common.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\frame.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\general_functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\groupby.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\indexes.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\resample.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\scalars.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\series.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\window.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\missing\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\missing\n",
      "  copying build\\lib\\pyspark\\pandas\\mlflow.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\namespace.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\numpy_compat.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\\plot\n",
      "  copying build\\lib\\pyspark\\pandas\\plot\\core.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\plot\n",
      "  copying build\\lib\\pyspark\\pandas\\plot\\matplotlib.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\plot\n",
      "  copying build\\lib\\pyspark\\pandas\\plot\\plotly.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\plot\n",
      "  copying build\\lib\\pyspark\\pandas\\plot\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\plot\n",
      "  copying build\\lib\\pyspark\\pandas\\resample.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\series.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\\spark\n",
      "  copying build\\lib\\pyspark\\pandas\\spark\\accessors.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\spark\n",
      "  copying build\\lib\\pyspark\\pandas\\spark\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\spark\n",
      "  copying build\\lib\\pyspark\\pandas\\spark\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\spark\n",
      "  copying build\\lib\\pyspark\\pandas\\spark\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\spark\n",
      "  copying build\\lib\\pyspark\\pandas\\sql_formatter.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\sql_processor.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\strings.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\supported_api_gen.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\\typedef\n",
      "  copying build\\lib\\pyspark\\pandas\\typedef\\typehints.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\typedef\n",
      "  copying build\\lib\\pyspark\\pandas\\typedef\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\typedef\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\pandas\\usage_logging\n",
      "  copying build\\lib\\pyspark\\pandas\\usage_logging\\usage_logger.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\usage_logging\n",
      "  copying build\\lib\\pyspark\\pandas\\usage_logging\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\\usage_logging\n",
      "  copying build\\lib\\pyspark\\pandas\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\window.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\_typing.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\pandas\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\pandas\n",
      "  copying build\\lib\\pyspark\\profiler.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\py.typed -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\python\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\python\\lib\n",
      "  copying build\\lib\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\python\\lib\n",
      "  copying build\\lib\\pyspark\\python\\lib\\pyspark.zip -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\python\\lib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\python\\pyspark\n",
      "  copying build\\lib\\pyspark\\python\\pyspark\\shell.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\python\\pyspark\n",
      "  copying build\\lib\\pyspark\\rdd.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\rddsampler.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\resource\n",
      "  copying build\\lib\\pyspark\\resource\\information.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\resource\n",
      "  copying build\\lib\\pyspark\\resource\\profile.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\resource\n",
      "  copying build\\lib\\pyspark\\resource\\requests.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\resource\n",
      "  copying build\\lib\\pyspark\\resource\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\resource\n",
      "  copying build\\lib\\pyspark\\resultiterable.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sbin\n",
      "  copying build\\lib\\pyspark\\sbin\\spark-config.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sbin\n",
      "  copying build\\lib\\pyspark\\sbin\\spark-daemon.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sbin\n",
      "  copying build\\lib\\pyspark\\sbin\\start-history-server.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sbin\n",
      "  copying build\\lib\\pyspark\\sbin\\stop-history-server.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sbin\n",
      "  copying build\\lib\\pyspark\\serializers.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\shell.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\shuffle.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\avro\n",
      "  copying build\\lib\\pyspark\\sql\\avro\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\avro\n",
      "  copying build\\lib\\pyspark\\sql\\avro\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\avro\n",
      "  copying build\\lib\\pyspark\\sql\\catalog.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\column.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\conf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\connect\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\connect\\avro\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\avro\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\avro\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\avro\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\avro\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\catalog.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\connect\\client\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\client\\artifact.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\client\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\client\\core.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\client\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\client\\reattach.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\client\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\client\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\client\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\column.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\conf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\conversion.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\dataframe.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\expressions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\group.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\plan.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\base_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\base_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\base_pb2_grpc.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\catalog_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\catalog_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\commands_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\commands_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\common_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\common_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\example_plugins_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\example_plugins_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\expressions_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\expressions_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\relations_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\relations_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\types_pb2.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\types_pb2.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\proto\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\proto\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\connect\\protobuf\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\protobuf\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\protobuf\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\protobuf\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\protobuf\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\readwriter.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\session.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\connect\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\streaming\\query.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\streaming\\readwriter.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\streaming\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\streaming\\worker\\foreach_batch_worker.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\streaming\\worker\\listener_worker.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\streaming\\worker\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\streaming\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\types.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\udf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\udtf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\window.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\_typing.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\connect\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\connect\n",
      "  copying build\\lib\\pyspark\\sql\\context.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\dataframe.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\group.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\observation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\conversion.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\functions.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\group_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\map_ops.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\serializers.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\typehints.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\types.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\pandas\\_typing\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\\frame.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\\series.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\\__init__.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\_typing\\__init__.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\\_typing\n",
      "  copying build\\lib\\pyspark\\sql\\pandas\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\pandas\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\protobuf\n",
      "  copying build\\lib\\pyspark\\sql\\protobuf\\functions.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\protobuf\n",
      "  copying build\\lib\\pyspark\\sql\\protobuf\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\protobuf\n",
      "  copying build\\lib\\pyspark\\sql\\readwriter.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\session.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\sql_formatter.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\streaming\\listener.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\streaming\\query.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\streaming\\readwriter.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\streaming\\state.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\streaming\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\sql\\types.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\udf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\udtf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\window.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\_typing.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\sql\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\sql\n",
      "  copying build\\lib\\pyspark\\statcounter.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\status.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\storagelevel.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\streaming\n",
      "  copying build\\lib\\pyspark\\streaming\\context.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\streaming\n",
      "  copying build\\lib\\pyspark\\streaming\\dstream.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\streaming\n",
      "  copying build\\lib\\pyspark\\streaming\\kinesis.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\streaming\n",
      "  copying build\\lib\\pyspark\\streaming\\listener.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\streaming\n",
      "  copying build\\lib\\pyspark\\streaming\\util.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\streaming\n",
      "  copying build\\lib\\pyspark\\streaming\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\streaming\n",
      "  copying build\\lib\\pyspark\\taskcontext.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\connectutils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\mllibutils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\mlutils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\pandasutils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\sqlutils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\streamingutils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\testing\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\testing\n",
      "  copying build\\lib\\pyspark\\traceback_utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\util.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\version.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\worker.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\worker_util.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\_globals.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\_typing.pyi -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  running install_egg_info\n",
      "  Copying pyspark.egg-info to build\\bdist.win-amd64\\wheel\\.\\pyspark-3.5.2-py3.9.egg-info\n",
      "  running install_scripts\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\beeline -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\beeline.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\docker-image-tool.sh -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\find-spark-home -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\find-spark-home.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\find_spark_home.py -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\load-spark-env.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\load-spark-env.sh -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\pyspark -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\pyspark.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\pyspark2.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\run-example -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\run-example.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-class -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-class.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-class2.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-connect-shell -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-shell -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-shell.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-shell2.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-sql -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-sql.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-sql2.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-submit -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-submit.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\spark-submit2.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\sparkR -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\sparkR.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  copying build\\scripts-3.9\\sparkR2.cmd -> build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.data\\scripts\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark-3.5.2.dist-info\\WHEEL\n",
      "  creating 'C:\\Users\\HP\\AppData\\Local\\Temp\\pip-wheel-ckabd2gd\\pyspark-3.5.2-py2.py3-none-any.whl' and adding 'build\\bdist.win-amd64\\wheel' to it\n",
      "  adding 'pyspark/__init__.py'\n",
      "  adding 'pyspark/_globals.py'\n",
      "  adding 'pyspark/_typing.pyi'\n",
      "  adding 'pyspark/accumulators.py'\n",
      "  adding 'pyspark/broadcast.py'\n",
      "  adding 'pyspark/conf.py'\n",
      "  adding 'pyspark/context.py'\n",
      "  adding 'pyspark/daemon.py'\n",
      "  adding 'pyspark/files.py'\n",
      "  adding 'pyspark/find_spark_home.py'\n",
      "  adding 'pyspark/install.py'\n",
      "  adding 'pyspark/instrumentation_utils.py'\n",
      "  adding 'pyspark/java_gateway.py'\n",
      "  adding 'pyspark/join.py'\n",
      "  adding 'pyspark/profiler.py'\n",
      "  adding 'pyspark/py.typed'\n",
      "  adding 'pyspark/rdd.py'\n",
      "  adding 'pyspark/rddsampler.py'\n",
      "  adding 'pyspark/resultiterable.py'\n",
      "  adding 'pyspark/serializers.py'\n",
      "  adding 'pyspark/shell.py'\n",
      "  adding 'pyspark/shuffle.py'\n",
      "  adding 'pyspark/statcounter.py'\n",
      "  adding 'pyspark/status.py'\n",
      "  adding 'pyspark/storagelevel.py'\n",
      "  adding 'pyspark/taskcontext.py'\n",
      "  adding 'pyspark/traceback_utils.py'\n",
      "  adding 'pyspark/util.py'\n",
      "  adding 'pyspark/version.py'\n",
      "  adding 'pyspark/worker.py'\n",
      "  adding 'pyspark/worker_util.py'\n",
      "  adding 'pyspark/bin/beeline'\n",
      "  adding 'pyspark/bin/beeline.cmd'\n",
      "  adding 'pyspark/bin/docker-image-tool.sh'\n",
      "  adding 'pyspark/bin/find-spark-home'\n",
      "  adding 'pyspark/bin/find-spark-home.cmd'\n",
      "  adding 'pyspark/bin/load-spark-env.cmd'\n",
      "  adding 'pyspark/bin/load-spark-env.sh'\n",
      "  adding 'pyspark/bin/pyspark'\n",
      "  adding 'pyspark/bin/pyspark.cmd'\n",
      "  adding 'pyspark/bin/pyspark2.cmd'\n",
      "  adding 'pyspark/bin/run-example'\n",
      "  adding 'pyspark/bin/run-example.cmd'\n",
      "  adding 'pyspark/bin/spark-class'\n",
      "  adding 'pyspark/bin/spark-class.cmd'\n",
      "  adding 'pyspark/bin/spark-class2.cmd'\n",
      "  adding 'pyspark/bin/spark-connect-shell'\n",
      "  adding 'pyspark/bin/spark-shell'\n",
      "  adding 'pyspark/bin/spark-shell.cmd'\n",
      "  adding 'pyspark/bin/spark-shell2.cmd'\n",
      "  adding 'pyspark/bin/spark-sql'\n",
      "  adding 'pyspark/bin/spark-sql.cmd'\n",
      "  adding 'pyspark/bin/spark-sql2.cmd'\n",
      "  adding 'pyspark/bin/spark-submit'\n",
      "  adding 'pyspark/bin/spark-submit.cmd'\n",
      "  adding 'pyspark/bin/spark-submit2.cmd'\n",
      "  adding 'pyspark/bin/sparkR'\n",
      "  adding 'pyspark/bin/sparkR.cmd'\n",
      "  adding 'pyspark/bin/sparkR2.cmd'\n",
      "  adding 'pyspark/cloudpickle/__init__.py'\n",
      "  adding 'pyspark/cloudpickle/cloudpickle.py'\n",
      "  adding 'pyspark/cloudpickle/cloudpickle_fast.py'\n",
      "  adding 'pyspark/cloudpickle/compat.py'\n",
      "  adding 'pyspark/data/artifact-tests/crc/junitLargeJar.txt'\n",
      "  adding 'pyspark/data/artifact-tests/crc/smallJar.txt'\n",
      "  adding 'pyspark/data/graphx/followers.txt'\n",
      "  adding 'pyspark/data/graphx/users.txt'\n",
      "  adding 'pyspark/data/mllib/gmm_data.txt'\n",
      "  adding 'pyspark/data/mllib/kmeans_data.txt'\n",
      "  adding 'pyspark/data/mllib/pagerank_data.txt'\n",
      "  adding 'pyspark/data/mllib/pic_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_binary_classification_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_fpgrowth.txt'\n",
      "  adding 'pyspark/data/mllib/sample_isotonic_regression_libsvm_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_kmeans_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_lda_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_lda_libsvm_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_libsvm_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_linear_regression_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_movielens_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_multiclass_classification_data.txt'\n",
      "  adding 'pyspark/data/mllib/sample_svm_data.txt'\n",
      "  adding 'pyspark/data/mllib/streaming_kmeans_data_test.txt'\n",
      "  adding 'pyspark/data/mllib/als/sample_movielens_ratings.txt'\n",
      "  adding 'pyspark/data/mllib/als/test.data'\n",
      "  adding 'pyspark/data/mllib/images/license.txt'\n",
      "  adding 'pyspark/data/mllib/images/origin/license.txt'\n",
      "  adding 'pyspark/data/mllib/images/origin/kittens/not-image.txt'\n",
      "  adding 'pyspark/data/mllib/ridge-data/lpsa.data'\n",
      "  adding 'pyspark/data/streaming/AFINN-111.txt'\n",
      "  adding 'pyspark/errors/__init__.py'\n",
      "  adding 'pyspark/errors/error_classes.py'\n",
      "  adding 'pyspark/errors/utils.py'\n",
      "  adding 'pyspark/errors/exceptions/__init__.py'\n",
      "  adding 'pyspark/errors/exceptions/base.py'\n",
      "  adding 'pyspark/errors/exceptions/captured.py'\n",
      "  adding 'pyspark/errors/exceptions/connect.py'\n",
      "  adding 'pyspark/examples/src/main/python/__init__.py'\n",
      "  adding 'pyspark/examples/src/main/python/als.py'\n",
      "  adding 'pyspark/examples/src/main/python/avro_inputformat.py'\n",
      "  adding 'pyspark/examples/src/main/python/kmeans.py'\n",
      "  adding 'pyspark/examples/src/main/python/logistic_regression.py'\n",
      "  adding 'pyspark/examples/src/main/python/pagerank.py'\n",
      "  adding 'pyspark/examples/src/main/python/parquet_inputformat.py'\n",
      "  adding 'pyspark/examples/src/main/python/pi.py'\n",
      "  adding 'pyspark/examples/src/main/python/sort.py'\n",
      "  adding 'pyspark/examples/src/main/python/status_api_demo.py'\n",
      "  adding 'pyspark/examples/src/main/python/transitive_closure.py'\n",
      "  adding 'pyspark/examples/src/main/python/wordcount.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/aft_survival_regression.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/als_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/binarizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/bisecting_k_means_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/bucketizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/chi_square_test_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/chisq_selector_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/correlation_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/count_vectorizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/cross_validator.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/dataframe_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/dct_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/decision_tree_classification_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/decision_tree_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/elementwise_product_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/estimator_transformer_param_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/feature_hasher_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/fm_classifier_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/fm_regressor_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/fpgrowth_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/gaussian_mixture_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/generalized_linear_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/imputer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/index_to_string_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/interaction_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/isotonic_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/kmeans_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/lda_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/linear_regression_with_elastic_net.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/linearsvc.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/logistic_regression_summary_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/logistic_regression_with_elastic_net.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/max_abs_scaler_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/min_hash_lsh_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/min_max_scaler_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/multilayer_perceptron_classification.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/n_gram_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/naive_bayes_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/normalizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/one_vs_rest_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/onehot_encoder_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/pca_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/pipeline_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/polynomial_expansion_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/power_iteration_clustering_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/prefixspan_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/quantile_discretizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/random_forest_classifier_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/random_forest_regressor_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/rformula_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/robust_scaler_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/sql_transformer.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/standard_scaler_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/stopwords_remover_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/string_indexer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/summarizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/tf_idf_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/tokenizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/train_validation_split.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/univariate_feature_selector_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/variance_threshold_selector_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/vector_assembler_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/vector_indexer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/vector_size_hint_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/vector_slicer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/ml/word2vec_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/__init__.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/binary_classification_metrics_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/bisecting_k_means_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/correlations.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/correlations_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/decision_tree_classification_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/decision_tree_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/elementwise_product_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/fpgrowth_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/gaussian_mixture_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/gaussian_mixture_model.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/gradient_boosting_classification_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/gradient_boosting_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/hypothesis_testing_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/isotonic_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/k_means_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/kernel_density_estimation_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/kmeans.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/linear_regression_with_sgd_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/logistic_regression.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/multi_class_metrics_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/multi_label_metrics_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/naive_bayes_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/normalizer_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/pca_rowmatrix_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/power_iteration_clustering_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/random_forest_classification_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/random_forest_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/random_rdd_generation.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/ranking_metrics_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/recommendation_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/regression_metrics_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/sampled_rdds.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/standard_scaler_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/stratified_sampling_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/streaming_k_means_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/streaming_linear_regression_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/summary_statistics_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/svd_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/svm_with_sgd_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/tf_idf_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/word2vec.py'\n",
      "  adding 'pyspark/examples/src/main/python/mllib/word2vec_example.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/__init__.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/arrow.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/basic.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/datasource.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/hive.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/udtf.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/streaming/structured_network_wordcount.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/streaming/structured_network_wordcount_session_window.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py'\n",
      "  adding 'pyspark/examples/src/main/python/sql/streaming/structured_sessionization.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/__init__.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/hdfs_wordcount.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/network_wordcount.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/network_wordjoinsentiments.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/queue_stream.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/recoverable_network_wordcount.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/sql_network_wordcount.py'\n",
      "  adding 'pyspark/examples/src/main/python/streaming/stateful_network_wordcount.py'\n",
      "  adding 'pyspark/jars/HikariCP-2.5.1.jar'\n",
      "  adding 'pyspark/jars/JLargeArrays-1.5.jar'\n",
      "  adding 'pyspark/jars/JTransforms-3.1.jar'\n",
      "  adding 'pyspark/jars/RoaringBitmap-0.9.45.jar'\n",
      "  adding 'pyspark/jars/ST4-4.0.4.jar'\n",
      "  adding 'pyspark/jars/activation-1.1.1.jar'\n",
      "  adding 'pyspark/jars/aircompressor-0.27.jar'\n",
      "  adding 'pyspark/jars/algebra_2.12-2.0.1.jar'\n",
      "  adding 'pyspark/jars/annotations-17.0.0.jar'\n",
      "  adding 'pyspark/jars/antlr-runtime-3.5.2.jar'\n",
      "  adding 'pyspark/jars/antlr4-runtime-4.9.3.jar'\n",
      "  adding 'pyspark/jars/aopalliance-repackaged-2.6.1.jar'\n",
      "  adding 'pyspark/jars/arpack-3.0.3.jar'\n",
      "  adding 'pyspark/jars/arpack_combined_all-0.1.jar'\n",
      "  adding 'pyspark/jars/arrow-format-12.0.1.jar'\n",
      "  adding 'pyspark/jars/arrow-memory-core-12.0.1.jar'\n",
      "  adding 'pyspark/jars/arrow-memory-netty-12.0.1.jar'\n",
      "  adding 'pyspark/jars/arrow-vector-12.0.1.jar'\n",
      "  adding 'pyspark/jars/audience-annotations-0.5.0.jar'\n",
      "  adding 'pyspark/jars/avro-1.11.2.jar'\n",
      "  adding 'pyspark/jars/avro-ipc-1.11.2.jar'\n",
      "  adding 'pyspark/jars/avro-mapred-1.11.2.jar'\n",
      "  adding 'pyspark/jars/blas-3.0.3.jar'\n",
      "  adding 'pyspark/jars/bonecp-0.8.0.RELEASE.jar'\n",
      "  adding 'pyspark/jars/breeze-macros_2.12-2.1.0.jar'\n",
      "  adding 'pyspark/jars/breeze_2.12-2.1.0.jar'\n",
      "  adding 'pyspark/jars/cats-kernel_2.12-2.1.1.jar'\n",
      "  adding 'pyspark/jars/chill-java-0.10.0.jar'\n",
      "  adding 'pyspark/jars/chill_2.12-0.10.0.jar'\n",
      "  adding 'pyspark/jars/commons-cli-1.5.0.jar'\n",
      "  adding 'pyspark/jars/commons-codec-1.16.1.jar'\n",
      "  adding 'pyspark/jars/commons-collections-3.2.2.jar'\n",
      "  adding 'pyspark/jars/commons-collections4-4.4.jar'\n",
      "  adding 'pyspark/jars/commons-compiler-3.1.9.jar'\n",
      "  adding 'pyspark/jars/commons-compress-1.23.0.jar'\n",
      "  adding 'pyspark/jars/commons-crypto-1.1.0.jar'\n",
      "  adding 'pyspark/jars/commons-dbcp-1.4.jar'\n",
      "  adding 'pyspark/jars/commons-io-2.16.1.jar'\n",
      "  adding 'pyspark/jars/commons-lang-2.6.jar'\n",
      "  adding 'pyspark/jars/commons-lang3-3.12.0.jar'\n",
      "  adding 'pyspark/jars/commons-logging-1.1.3.jar'\n",
      "  adding 'pyspark/jars/commons-math3-3.6.1.jar'\n",
      "  adding 'pyspark/jars/commons-pool-1.5.4.jar'\n",
      "  adding 'pyspark/jars/commons-text-1.10.0.jar'\n",
      "  adding 'pyspark/jars/compress-lzf-1.1.2.jar'\n",
      "  adding 'pyspark/jars/curator-client-2.13.0.jar'\n",
      "  adding 'pyspark/jars/curator-framework-2.13.0.jar'\n",
      "  adding 'pyspark/jars/curator-recipes-2.13.0.jar'\n",
      "  adding 'pyspark/jars/datanucleus-api-jdo-4.2.4.jar'\n",
      "  adding 'pyspark/jars/datanucleus-core-4.1.17.jar'\n",
      "  adding 'pyspark/jars/datanucleus-rdbms-4.1.19.jar'\n",
      "  adding 'pyspark/jars/datasketches-java-3.3.0.jar'\n",
      "  adding 'pyspark/jars/datasketches-memory-2.1.0.jar'\n",
      "  adding 'pyspark/jars/derby-10.14.2.0.jar'\n",
      "  adding 'pyspark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar'\n",
      "  adding 'pyspark/jars/flatbuffers-java-1.12.0.jar'\n",
      "  adding 'pyspark/jars/gson-2.2.4.jar'\n",
      "  adding 'pyspark/jars/guava-14.0.1.jar'\n",
      "  adding 'pyspark/jars/hadoop-client-api-3.3.4.jar'\n",
      "  adding 'pyspark/jars/hadoop-client-runtime-3.3.4.jar'\n",
      "  adding 'pyspark/jars/hadoop-shaded-guava-1.1.1.jar'\n",
      "  adding 'pyspark/jars/hadoop-yarn-server-web-proxy-3.3.4.jar'\n",
      "  adding 'pyspark/jars/hive-beeline-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-cli-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-common-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-exec-2.3.9-core.jar'\n",
      "  adding 'pyspark/jars/hive-jdbc-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-llap-common-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-metastore-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-serde-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-service-rpc-3.1.3.jar'\n",
      "  adding 'pyspark/jars/hive-shims-0.23-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-shims-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-shims-common-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-shims-scheduler-2.3.9.jar'\n",
      "  adding 'pyspark/jars/hive-storage-api-2.8.1.jar'\n",
      "  adding 'pyspark/jars/hk2-api-2.6.1.jar'\n",
      "  adding 'pyspark/jars/hk2-locator-2.6.1.jar'\n",
      "  adding 'pyspark/jars/hk2-utils-2.6.1.jar'\n",
      "  adding 'pyspark/jars/httpclient-4.5.14.jar'\n",
      "  adding 'pyspark/jars/httpcore-4.4.16.jar'\n",
      "  adding 'pyspark/jars/istack-commons-runtime-3.0.8.jar'\n",
      "  adding 'pyspark/jars/ivy-2.5.1.jar'\n",
      "  adding 'pyspark/jars/jackson-annotations-2.15.2.jar'\n",
      "  adding 'pyspark/jars/jackson-core-2.15.2.jar'\n",
      "  adding 'pyspark/jars/jackson-core-asl-1.9.13.jar'\n",
      "  adding 'pyspark/jars/jackson-databind-2.15.2.jar'\n",
      "  adding 'pyspark/jars/jackson-dataformat-yaml-2.15.2.jar'\n",
      "  adding 'pyspark/jars/jackson-datatype-jsr310-2.15.2.jar'\n",
      "  adding 'pyspark/jars/jackson-mapper-asl-1.9.13.jar'\n",
      "  adding 'pyspark/jars/jackson-module-scala_2.12-2.15.2.jar'\n",
      "  adding 'pyspark/jars/jakarta.annotation-api-1.3.5.jar'\n",
      "  adding 'pyspark/jars/jakarta.inject-2.6.1.jar'\n",
      "  adding 'pyspark/jars/jakarta.servlet-api-4.0.3.jar'\n",
      "  adding 'pyspark/jars/jakarta.validation-api-2.0.2.jar'\n",
      "  adding 'pyspark/jars/jakarta.ws.rs-api-2.1.6.jar'\n",
      "  adding 'pyspark/jars/jakarta.xml.bind-api-2.3.2.jar'\n",
      "  adding 'pyspark/jars/janino-3.1.9.jar'\n",
      "  adding 'pyspark/jars/javassist-3.29.2-GA.jar'\n",
      "  adding 'pyspark/jars/javax.jdo-3.2.0-m3.jar'\n",
      "  adding 'pyspark/jars/javolution-5.5.1.jar'\n",
      "  adding 'pyspark/jars/jaxb-runtime-2.3.2.jar'\n",
      "  adding 'pyspark/jars/jcl-over-slf4j-2.0.7.jar'\n",
      "  adding 'pyspark/jars/jdo-api-3.0.1.jar'\n",
      "  adding 'pyspark/jars/jersey-client-2.40.jar'\n",
      "  adding 'pyspark/jars/jersey-common-2.40.jar'\n",
      "  adding 'pyspark/jars/jersey-container-servlet-2.40.jar'\n",
      "  adding 'pyspark/jars/jersey-container-servlet-core-2.40.jar'\n",
      "  adding 'pyspark/jars/jersey-hk2-2.40.jar'\n",
      "  adding 'pyspark/jars/jersey-server-2.40.jar'\n",
      "  adding 'pyspark/jars/jline-2.14.6.jar'\n",
      "  adding 'pyspark/jars/joda-time-2.12.5.jar'\n",
      "  adding 'pyspark/jars/jodd-core-3.5.2.jar'\n",
      "  adding 'pyspark/jars/jpam-1.1.jar'\n",
      "  adding 'pyspark/jars/json-1.8.jar'\n",
      "  adding 'pyspark/jars/json4s-ast_2.12-3.7.0-M11.jar'\n",
      "  adding 'pyspark/jars/json4s-core_2.12-3.7.0-M11.jar'\n",
      "  adding 'pyspark/jars/json4s-jackson_2.12-3.7.0-M11.jar'\n",
      "  adding 'pyspark/jars/json4s-scalap_2.12-3.7.0-M11.jar'\n",
      "  adding 'pyspark/jars/jsr305-3.0.0.jar'\n",
      "  adding 'pyspark/jars/jta-1.1.jar'\n",
      "  adding 'pyspark/jars/jul-to-slf4j-2.0.7.jar'\n",
      "  adding 'pyspark/jars/kryo-shaded-4.0.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-client-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-client-api-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-httpclient-okhttp-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-admissionregistration-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-apiextensions-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-apps-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-autoscaling-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-batch-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-certificates-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-common-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-coordination-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-core-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-discovery-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-events-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-extensions-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-flowcontrol-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-gatewayapi-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-metrics-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-networking-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-node-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-policy-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-rbac-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-resource-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-scheduling-6.7.2.jar'\n",
      "  adding 'pyspark/jars/kubernetes-model-storageclass-6.7.2.jar'\n",
      "  adding 'pyspark/jars/lapack-3.0.3.jar'\n",
      "  adding 'pyspark/jars/leveldbjni-all-1.8.jar'\n",
      "  adding 'pyspark/jars/libfb303-0.9.3.jar'\n",
      "  adding 'pyspark/jars/libthrift-0.12.0.jar'\n",
      "  adding 'pyspark/jars/log4j-1.2-api-2.20.0.jar'\n",
      "  adding 'pyspark/jars/log4j-api-2.20.0.jar'\n",
      "  adding 'pyspark/jars/log4j-core-2.20.0.jar'\n",
      "  adding 'pyspark/jars/log4j-slf4j2-impl-2.20.0.jar'\n",
      "  adding 'pyspark/jars/logging-interceptor-3.12.12.jar'\n",
      "  adding 'pyspark/jars/lz4-java-1.8.0.jar'\n",
      "  adding 'pyspark/jars/mesos-1.4.3-shaded-protobuf.jar'\n",
      "  adding 'pyspark/jars/metrics-core-4.2.19.jar'\n",
      "  adding 'pyspark/jars/metrics-graphite-4.2.19.jar'\n",
      "  adding 'pyspark/jars/metrics-jmx-4.2.19.jar'\n",
      "  adding 'pyspark/jars/metrics-json-4.2.19.jar'\n",
      "  adding 'pyspark/jars/metrics-jvm-4.2.19.jar'\n",
      "  adding 'pyspark/jars/minlog-1.3.0.jar'\n",
      "  adding 'pyspark/jars/netty-all-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-buffer-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-codec-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-codec-http-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-codec-http2-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-codec-socks-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-common-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-handler-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-handler-proxy-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-resolver-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-transport-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-transport-classes-epoll-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-transport-classes-kqueue-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar'\n",
      "  adding 'pyspark/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar'\n",
      "  adding 'pyspark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar'\n",
      "  adding 'pyspark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar'\n",
      "  adding 'pyspark/jars/netty-transport-native-unix-common-4.1.96.Final.jar'\n",
      "  adding 'pyspark/jars/objenesis-3.3.jar'\n",
      "  adding 'pyspark/jars/okhttp-3.12.12.jar'\n",
      "  adding 'pyspark/jars/okio-1.15.0.jar'\n",
      "  adding 'pyspark/jars/opencsv-2.3.jar'\n",
      "  adding 'pyspark/jars/orc-core-1.9.4-shaded-protobuf.jar'\n",
      "  adding 'pyspark/jars/orc-mapreduce-1.9.4-shaded-protobuf.jar'\n",
      "  adding 'pyspark/jars/orc-shims-1.9.4.jar'\n",
      "  adding 'pyspark/jars/oro-2.0.8.jar'\n",
      "  adding 'pyspark/jars/osgi-resource-locator-1.0.3.jar'\n",
      "  adding 'pyspark/jars/paranamer-2.8.jar'\n",
      "  adding 'pyspark/jars/parquet-column-1.13.1.jar'\n",
      "  adding 'pyspark/jars/parquet-common-1.13.1.jar'\n",
      "  adding 'pyspark/jars/parquet-encoding-1.13.1.jar'\n",
      "  adding 'pyspark/jars/parquet-format-structures-1.13.1.jar'\n",
      "  adding 'pyspark/jars/parquet-hadoop-1.13.1.jar'\n",
      "  adding 'pyspark/jars/parquet-jackson-1.13.1.jar'\n",
      "  adding 'pyspark/jars/pickle-1.3.jar'\n",
      "  adding 'pyspark/jars/py4j-0.10.9.7.jar'\n",
      "  adding 'pyspark/jars/rocksdbjni-8.3.2.jar'\n",
      "  adding 'pyspark/jars/scala-collection-compat_2.12-2.7.0.jar'\n",
      "  adding 'pyspark/jars/scala-compiler-2.12.18.jar'\n",
      "  adding 'pyspark/jars/scala-library-2.12.18.jar'\n",
      "  adding 'pyspark/jars/scala-parser-combinators_2.12-2.3.0.jar'\n",
      "  adding 'pyspark/jars/scala-reflect-2.12.18.jar'\n",
      "  adding 'pyspark/jars/scala-xml_2.12-2.1.0.jar'\n",
      "  adding 'pyspark/jars/shims-0.9.45.jar'\n",
      "  adding 'pyspark/jars/slf4j-api-2.0.7.jar'\n",
      "  adding 'pyspark/jars/snakeyaml-2.0.jar'\n",
      "  adding 'pyspark/jars/snakeyaml-engine-2.6.jar'\n",
      "  adding 'pyspark/jars/snappy-java-1.1.10.5.jar'\n",
      "  adding 'pyspark/jars/spark-catalyst_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-common-utils_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-core_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-graphx_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-hive-thriftserver_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-hive_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-kubernetes_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-kvstore_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-launcher_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-mesos_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-mllib-local_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-mllib_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-network-common_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-network-shuffle_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-repl_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-sketch_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-sql-api_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-sql_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-streaming_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-tags_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-unsafe_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spark-yarn_2.12-3.5.2.jar'\n",
      "  adding 'pyspark/jars/spire-macros_2.12-0.17.0.jar'\n",
      "  adding 'pyspark/jars/spire-platform_2.12-0.17.0.jar'\n",
      "  adding 'pyspark/jars/spire-util_2.12-0.17.0.jar'\n",
      "  adding 'pyspark/jars/spire_2.12-0.17.0.jar'\n",
      "  adding 'pyspark/jars/stax-api-1.0.1.jar'\n",
      "  adding 'pyspark/jars/stream-2.9.6.jar'\n",
      "  adding 'pyspark/jars/super-csv-2.2.0.jar'\n",
      "  adding 'pyspark/jars/threeten-extra-1.7.1.jar'\n",
      "  adding 'pyspark/jars/tink-1.9.0.jar'\n",
      "  adding 'pyspark/jars/transaction-api-1.1.jar'\n",
      "  adding 'pyspark/jars/univocity-parsers-2.9.1.jar'\n",
      "  adding 'pyspark/jars/xbean-asm9-shaded-4.23.jar'\n",
      "  adding 'pyspark/jars/xz-1.9.jar'\n",
      "  adding 'pyspark/jars/zjsonpatch-0.3.0.jar'\n",
      "  adding 'pyspark/jars/zookeeper-3.6.3.jar'\n",
      "  adding 'pyspark/jars/zookeeper-jute-3.6.3.jar'\n",
      "  adding 'pyspark/jars/zstd-jni-1.5.5-4.jar'\n",
      "  adding 'pyspark/licenses/LICENSE-AnchorJS.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-CC0.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-bootstrap.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-cloudpickle.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-d3.min.js.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-dagre-d3.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-datatables.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-graphlib-dot.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-jdom.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-join.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-jquery.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-json-formatter.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-matchMedia-polyfill.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-modernizr.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-mustache.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-py4j.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-respond.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-sbt-launch-lib.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-sorttable.js.txt'\n",
      "  adding 'pyspark/licenses/LICENSE-vis-timeline.txt'\n",
      "  adding 'pyspark/ml/__init__.py'\n",
      "  adding 'pyspark/ml/_typing.pyi'\n",
      "  adding 'pyspark/ml/base.py'\n",
      "  adding 'pyspark/ml/classification.py'\n",
      "  adding 'pyspark/ml/clustering.py'\n",
      "  adding 'pyspark/ml/common.py'\n",
      "  adding 'pyspark/ml/dl_util.py'\n",
      "  adding 'pyspark/ml/evaluation.py'\n",
      "  adding 'pyspark/ml/feature.py'\n",
      "  adding 'pyspark/ml/fpm.py'\n",
      "  adding 'pyspark/ml/functions.py'\n",
      "  adding 'pyspark/ml/image.py'\n",
      "  adding 'pyspark/ml/model_cache.py'\n",
      "  adding 'pyspark/ml/pipeline.py'\n",
      "  adding 'pyspark/ml/recommendation.py'\n",
      "  adding 'pyspark/ml/regression.py'\n",
      "  adding 'pyspark/ml/stat.py'\n",
      "  adding 'pyspark/ml/tree.py'\n",
      "  adding 'pyspark/ml/tuning.py'\n",
      "  adding 'pyspark/ml/util.py'\n",
      "  adding 'pyspark/ml/wrapper.py'\n",
      "  adding 'pyspark/ml/connect/__init__.py'\n",
      "  adding 'pyspark/ml/connect/base.py'\n",
      "  adding 'pyspark/ml/connect/classification.py'\n",
      "  adding 'pyspark/ml/connect/evaluation.py'\n",
      "  adding 'pyspark/ml/connect/feature.py'\n",
      "  adding 'pyspark/ml/connect/functions.py'\n",
      "  adding 'pyspark/ml/connect/io_utils.py'\n",
      "  adding 'pyspark/ml/connect/pipeline.py'\n",
      "  adding 'pyspark/ml/connect/summarizer.py'\n",
      "  adding 'pyspark/ml/connect/tuning.py'\n",
      "  adding 'pyspark/ml/connect/util.py'\n",
      "  adding 'pyspark/ml/deepspeed/__init__.py'\n",
      "  adding 'pyspark/ml/deepspeed/deepspeed_distributor.py'\n",
      "  adding 'pyspark/ml/linalg/__init__.py'\n",
      "  adding 'pyspark/ml/param/__init__.py'\n",
      "  adding 'pyspark/ml/param/_shared_params_code_gen.py'\n",
      "  adding 'pyspark/ml/param/shared.py'\n",
      "  adding 'pyspark/ml/torch/__init__.py'\n",
      "  adding 'pyspark/ml/torch/data.py'\n",
      "  adding 'pyspark/ml/torch/distributor.py'\n",
      "  adding 'pyspark/ml/torch/log_communication.py'\n",
      "  adding 'pyspark/ml/torch/torch_run_process_wrapper.py'\n",
      "  adding 'pyspark/mllib/__init__.py'\n",
      "  adding 'pyspark/mllib/_typing.pyi'\n",
      "  adding 'pyspark/mllib/classification.py'\n",
      "  adding 'pyspark/mllib/clustering.py'\n",
      "  adding 'pyspark/mllib/common.py'\n",
      "  adding 'pyspark/mllib/evaluation.py'\n",
      "  adding 'pyspark/mllib/feature.py'\n",
      "  adding 'pyspark/mllib/fpm.py'\n",
      "  adding 'pyspark/mllib/random.py'\n",
      "  adding 'pyspark/mllib/recommendation.py'\n",
      "  adding 'pyspark/mllib/regression.py'\n",
      "  adding 'pyspark/mllib/tree.py'\n",
      "  adding 'pyspark/mllib/util.py'\n",
      "  adding 'pyspark/mllib/linalg/__init__.py'\n",
      "  adding 'pyspark/mllib/linalg/distributed.py'\n",
      "  adding 'pyspark/mllib/stat/KernelDensity.py'\n",
      "  adding 'pyspark/mllib/stat/__init__.py'\n",
      "  adding 'pyspark/mllib/stat/_statistics.py'\n",
      "  adding 'pyspark/mllib/stat/distribution.py'\n",
      "  adding 'pyspark/mllib/stat/test.py'\n",
      "  adding 'pyspark/pandas/__init__.py'\n",
      "  adding 'pyspark/pandas/_typing.py'\n",
      "  adding 'pyspark/pandas/accessors.py'\n",
      "  adding 'pyspark/pandas/base.py'\n",
      "  adding 'pyspark/pandas/categorical.py'\n",
      "  adding 'pyspark/pandas/config.py'\n",
      "  adding 'pyspark/pandas/correlation.py'\n",
      "  adding 'pyspark/pandas/datetimes.py'\n",
      "  adding 'pyspark/pandas/exceptions.py'\n",
      "  adding 'pyspark/pandas/extensions.py'\n",
      "  adding 'pyspark/pandas/frame.py'\n",
      "  adding 'pyspark/pandas/generic.py'\n",
      "  adding 'pyspark/pandas/groupby.py'\n",
      "  adding 'pyspark/pandas/indexing.py'\n",
      "  adding 'pyspark/pandas/internal.py'\n",
      "  adding 'pyspark/pandas/mlflow.py'\n",
      "  adding 'pyspark/pandas/namespace.py'\n",
      "  adding 'pyspark/pandas/numpy_compat.py'\n",
      "  adding 'pyspark/pandas/resample.py'\n",
      "  adding 'pyspark/pandas/series.py'\n",
      "  adding 'pyspark/pandas/sql_formatter.py'\n",
      "  adding 'pyspark/pandas/sql_processor.py'\n",
      "  adding 'pyspark/pandas/strings.py'\n",
      "  adding 'pyspark/pandas/supported_api_gen.py'\n",
      "  adding 'pyspark/pandas/utils.py'\n",
      "  adding 'pyspark/pandas/window.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/__init__.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/base.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/binary_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/boolean_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/categorical_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/complex_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/date_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/datetime_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/null_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/num_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/string_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/timedelta_ops.py'\n",
      "  adding 'pyspark/pandas/data_type_ops/udt_ops.py'\n",
      "  adding 'pyspark/pandas/indexes/__init__.py'\n",
      "  adding 'pyspark/pandas/indexes/base.py'\n",
      "  adding 'pyspark/pandas/indexes/category.py'\n",
      "  adding 'pyspark/pandas/indexes/datetimes.py'\n",
      "  adding 'pyspark/pandas/indexes/multi.py'\n",
      "  adding 'pyspark/pandas/indexes/numeric.py'\n",
      "  adding 'pyspark/pandas/indexes/timedelta.py'\n",
      "  adding 'pyspark/pandas/missing/__init__.py'\n",
      "  adding 'pyspark/pandas/missing/common.py'\n",
      "  adding 'pyspark/pandas/missing/frame.py'\n",
      "  adding 'pyspark/pandas/missing/general_functions.py'\n",
      "  adding 'pyspark/pandas/missing/groupby.py'\n",
      "  adding 'pyspark/pandas/missing/indexes.py'\n",
      "  adding 'pyspark/pandas/missing/resample.py'\n",
      "  adding 'pyspark/pandas/missing/scalars.py'\n",
      "  adding 'pyspark/pandas/missing/series.py'\n",
      "  adding 'pyspark/pandas/missing/window.py'\n",
      "  adding 'pyspark/pandas/plot/__init__.py'\n",
      "  adding 'pyspark/pandas/plot/core.py'\n",
      "  adding 'pyspark/pandas/plot/matplotlib.py'\n",
      "  adding 'pyspark/pandas/plot/plotly.py'\n",
      "  adding 'pyspark/pandas/spark/__init__.py'\n",
      "  adding 'pyspark/pandas/spark/accessors.py'\n",
      "  adding 'pyspark/pandas/spark/functions.py'\n",
      "  adding 'pyspark/pandas/spark/utils.py'\n",
      "  adding 'pyspark/pandas/typedef/__init__.py'\n",
      "  adding 'pyspark/pandas/typedef/typehints.py'\n",
      "  adding 'pyspark/pandas/usage_logging/__init__.py'\n",
      "  adding 'pyspark/pandas/usage_logging/usage_logger.py'\n",
      "  adding 'pyspark/python/lib/py4j-0.10.9.7-src.zip'\n",
      "  adding 'pyspark/python/lib/pyspark.zip'\n",
      "  adding 'pyspark/python/pyspark/shell.py'\n",
      "  adding 'pyspark/resource/__init__.py'\n",
      "  adding 'pyspark/resource/information.py'\n",
      "  adding 'pyspark/resource/profile.py'\n",
      "  adding 'pyspark/resource/requests.py'\n",
      "  adding 'pyspark/sbin/spark-config.sh'\n",
      "  adding 'pyspark/sbin/spark-daemon.sh'\n",
      "  adding 'pyspark/sbin/start-history-server.sh'\n",
      "  adding 'pyspark/sbin/stop-history-server.sh'\n",
      "  adding 'pyspark/sql/__init__.py'\n",
      "  adding 'pyspark/sql/_typing.pyi'\n",
      "  adding 'pyspark/sql/catalog.py'\n",
      "  adding 'pyspark/sql/column.py'\n",
      "  adding 'pyspark/sql/conf.py'\n",
      "  adding 'pyspark/sql/context.py'\n",
      "  adding 'pyspark/sql/dataframe.py'\n",
      "  adding 'pyspark/sql/functions.py'\n",
      "  adding 'pyspark/sql/group.py'\n",
      "  adding 'pyspark/sql/observation.py'\n",
      "  adding 'pyspark/sql/readwriter.py'\n",
      "  adding 'pyspark/sql/session.py'\n",
      "  adding 'pyspark/sql/sql_formatter.py'\n",
      "  adding 'pyspark/sql/types.py'\n",
      "  adding 'pyspark/sql/udf.py'\n",
      "  adding 'pyspark/sql/udtf.py'\n",
      "  adding 'pyspark/sql/utils.py'\n",
      "  adding 'pyspark/sql/window.py'\n",
      "  adding 'pyspark/sql/avro/__init__.py'\n",
      "  adding 'pyspark/sql/avro/functions.py'\n",
      "  adding 'pyspark/sql/connect/__init__.py'\n",
      "  adding 'pyspark/sql/connect/_typing.py'\n",
      "  adding 'pyspark/sql/connect/catalog.py'\n",
      "  adding 'pyspark/sql/connect/column.py'\n",
      "  adding 'pyspark/sql/connect/conf.py'\n",
      "  adding 'pyspark/sql/connect/conversion.py'\n",
      "  adding 'pyspark/sql/connect/dataframe.py'\n",
      "  adding 'pyspark/sql/connect/expressions.py'\n",
      "  adding 'pyspark/sql/connect/functions.py'\n",
      "  adding 'pyspark/sql/connect/group.py'\n",
      "  adding 'pyspark/sql/connect/plan.py'\n",
      "  adding 'pyspark/sql/connect/readwriter.py'\n",
      "  adding 'pyspark/sql/connect/session.py'\n",
      "  adding 'pyspark/sql/connect/types.py'\n",
      "  adding 'pyspark/sql/connect/udf.py'\n",
      "  adding 'pyspark/sql/connect/udtf.py'\n",
      "  adding 'pyspark/sql/connect/utils.py'\n",
      "  adding 'pyspark/sql/connect/window.py'\n",
      "  adding 'pyspark/sql/connect/avro/__init__.py'\n",
      "  adding 'pyspark/sql/connect/avro/functions.py'\n",
      "  adding 'pyspark/sql/connect/client/__init__.py'\n",
      "  adding 'pyspark/sql/connect/client/artifact.py'\n",
      "  adding 'pyspark/sql/connect/client/core.py'\n",
      "  adding 'pyspark/sql/connect/client/reattach.py'\n",
      "  adding 'pyspark/sql/connect/proto/__init__.py'\n",
      "  adding 'pyspark/sql/connect/proto/base_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/base_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/proto/base_pb2_grpc.py'\n",
      "  adding 'pyspark/sql/connect/proto/catalog_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/catalog_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/proto/commands_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/commands_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/proto/common_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/common_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/proto/example_plugins_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/example_plugins_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/proto/expressions_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/expressions_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/proto/relations_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/relations_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/proto/types_pb2.py'\n",
      "  adding 'pyspark/sql/connect/proto/types_pb2.pyi'\n",
      "  adding 'pyspark/sql/connect/protobuf/__init__.py'\n",
      "  adding 'pyspark/sql/connect/protobuf/functions.py'\n",
      "  adding 'pyspark/sql/connect/streaming/__init__.py'\n",
      "  adding 'pyspark/sql/connect/streaming/query.py'\n",
      "  adding 'pyspark/sql/connect/streaming/readwriter.py'\n",
      "  adding 'pyspark/sql/connect/streaming/worker/__init__.py'\n",
      "  adding 'pyspark/sql/connect/streaming/worker/foreach_batch_worker.py'\n",
      "  adding 'pyspark/sql/connect/streaming/worker/listener_worker.py'\n",
      "  adding 'pyspark/sql/pandas/__init__.py'\n",
      "  adding 'pyspark/sql/pandas/conversion.py'\n",
      "  adding 'pyspark/sql/pandas/functions.py'\n",
      "  adding 'pyspark/sql/pandas/functions.pyi'\n",
      "  adding 'pyspark/sql/pandas/group_ops.py'\n",
      "  adding 'pyspark/sql/pandas/map_ops.py'\n",
      "  adding 'pyspark/sql/pandas/serializers.py'\n",
      "  adding 'pyspark/sql/pandas/typehints.py'\n",
      "  adding 'pyspark/sql/pandas/types.py'\n",
      "  adding 'pyspark/sql/pandas/utils.py'\n",
      "  adding 'pyspark/sql/pandas/_typing/__init__.pyi'\n",
      "  adding 'pyspark/sql/pandas/_typing/protocols/__init__.pyi'\n",
      "  adding 'pyspark/sql/pandas/_typing/protocols/frame.pyi'\n",
      "  adding 'pyspark/sql/pandas/_typing/protocols/series.pyi'\n",
      "  adding 'pyspark/sql/protobuf/__init__.py'\n",
      "  adding 'pyspark/sql/protobuf/functions.py'\n",
      "  adding 'pyspark/sql/streaming/__init__.py'\n",
      "  adding 'pyspark/sql/streaming/listener.py'\n",
      "  adding 'pyspark/sql/streaming/query.py'\n",
      "  adding 'pyspark/sql/streaming/readwriter.py'\n",
      "  adding 'pyspark/sql/streaming/state.py'\n",
      "  adding 'pyspark/streaming/__init__.py'\n",
      "  adding 'pyspark/streaming/context.py'\n",
      "  adding 'pyspark/streaming/dstream.py'\n",
      "  adding 'pyspark/streaming/kinesis.py'\n",
      "  adding 'pyspark/streaming/listener.py'\n",
      "  adding 'pyspark/streaming/util.py'\n",
      "  adding 'pyspark/testing/__init__.py'\n",
      "  adding 'pyspark/testing/connectutils.py'\n",
      "  adding 'pyspark/testing/mllibutils.py'\n",
      "  adding 'pyspark/testing/mlutils.py'\n",
      "  adding 'pyspark/testing/pandasutils.py'\n",
      "  adding 'pyspark/testing/sqlutils.py'\n",
      "  adding 'pyspark/testing/streamingutils.py'\n",
      "  adding 'pyspark/testing/utils.py'\n",
      "  adding 'pyspark-3.5.2.data/scripts/beeline'\n",
      "  adding 'pyspark-3.5.2.data/scripts/beeline.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/docker-image-tool.sh'\n",
      "  adding 'pyspark-3.5.2.data/scripts/find-spark-home'\n",
      "  adding 'pyspark-3.5.2.data/scripts/find-spark-home.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/find_spark_home.py'\n",
      "  adding 'pyspark-3.5.2.data/scripts/load-spark-env.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/load-spark-env.sh'\n",
      "  adding 'pyspark-3.5.2.data/scripts/pyspark'\n",
      "  adding 'pyspark-3.5.2.data/scripts/pyspark.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/pyspark2.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/run-example'\n",
      "  adding 'pyspark-3.5.2.data/scripts/run-example.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-class'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-class.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-class2.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-connect-shell'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-shell'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-shell.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-shell2.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-sql'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-sql.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-sql2.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-submit'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-submit.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/spark-submit2.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/sparkR'\n",
      "  adding 'pyspark-3.5.2.data/scripts/sparkR.cmd'\n",
      "  adding 'pyspark-3.5.2.data/scripts/sparkR2.cmd'\n",
      "  adding 'pyspark-3.5.2.dist-info/METADATA'\n",
      "  adding 'pyspark-3.5.2.dist-info/WHEEL'\n",
      "  adding 'pyspark-3.5.2.dist-info/top_level.txt'\n",
      "  adding 'pyspark-3.5.2.dist-info/RECORD'\n",
      "  removing build\\bdist.win-amd64\\wheel\n",
      "  error: [WinError 32] The process cannot access the file because it is being used by another process: 'build\\\\bdist.win-amd64\\\\wheel\\\\pyspark\\\\jars\\\\hive-exec-2.3.9-core.jar'\n",
      "  [WinError 32] The process cannot access the file because it is being used by another process: 'build\\\\bdist.win-amd64\\\\wheel\\\\pyspark\\\\jars\\\\hive-exec-2.3.9-core.jar'\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyspark\n",
      "  DEPRECATION: pyspark was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python pyspark pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7637ac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Sent: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Send 10 transactions\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     send_transaction_data()\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Kafka Producer Configuration\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092', \n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "# Function to send transaction data\n",
    "def send_transaction_data():\n",
    "    transaction_data = {\n",
    "        \"transaction_id\": \"12345\",\n",
    "        \"user_id\": \"67890\",\n",
    "        \"transaction_amount\": 250.75,\n",
    "        \"timestamp\": \"2024-09-10 14:32:55\",\n",
    "        \"location\": \"New York, USA\",\n",
    "        \"device_id\": \"abc123\"\n",
    "    }\n",
    "    producer.send('transactions', transaction_data)\n",
    "    print(f\"Sent: {transaction_data}\")\n",
    "    producer.flush()\n",
    "\n",
    "# Simulating continuous data generation\n",
    "for _ in range(10):  # Send 10 transactions\n",
    "    send_transaction_data()\n",
    "    time.sleep(1)  # Pause for a second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a2373d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received transaction: {'transaction_id': '51831', 'user_id': '3033', 'transaction_amount': 375.68, 'timestamp': '2024-09-20 21:46:46', 'location': 'New York, USA', 'device_id': 'device_68'}\n",
      "Received transaction: {'transaction_id': '91939', 'user_id': '4276', 'transaction_amount': 236.31, 'timestamp': '2024-09-20 21:46:47', 'location': 'New York, USA', 'device_id': 'device_9'}\n",
      "Received transaction: {'transaction_id': '70826', 'user_id': '4911', 'transaction_amount': 37.92, 'timestamp': '2024-09-20 21:46:48', 'location': 'London, UK', 'device_id': 'device_17'}\n",
      "Received transaction: {'transaction_id': '81759', 'user_id': '6955', 'transaction_amount': 37.07, 'timestamp': '2024-09-20 21:46:49', 'location': 'Berlin, Germany', 'device_id': 'device_31'}\n",
      "Received transaction: {'transaction_id': '55204', 'user_id': '6253', 'transaction_amount': 123.77, 'timestamp': '2024-09-20 21:46:50', 'location': 'London, UK', 'device_id': 'device_93'}\n",
      "Received transaction: {'transaction_id': '60577', 'user_id': '1563', 'transaction_amount': 252.85, 'timestamp': '2024-09-20 21:46:51', 'location': 'London, UK', 'device_id': 'device_42'}\n",
      "Received transaction: {'transaction_id': '39758', 'user_id': '9358', 'transaction_amount': 480.49, 'timestamp': '2024-09-20 21:46:52', 'location': 'London, UK', 'device_id': 'device_43'}\n",
      "Received transaction: {'transaction_id': '39229', 'user_id': '8459', 'transaction_amount': 355.0, 'timestamp': '2024-09-20 21:46:53', 'location': 'New York, USA', 'device_id': 'device_51'}\n",
      "Received transaction: {'transaction_id': '88140', 'user_id': '9387', 'transaction_amount': 402.36, 'timestamp': '2024-09-20 21:46:54', 'location': 'New York, USA', 'device_id': 'device_100'}\n",
      "Received transaction: {'transaction_id': '34819', 'user_id': '6584', 'transaction_amount': 116.55, 'timestamp': '2024-09-20 21:46:55', 'location': 'London, UK', 'device_id': 'device_61'}\n",
      "Received transaction: {'transaction_id': '48639', 'user_id': '4014', 'transaction_amount': 132.71, 'timestamp': '2024-09-20 21:46:56', 'location': 'Berlin, Germany', 'device_id': 'device_87'}\n",
      "Received transaction: {'transaction_id': '77652', 'user_id': '4246', 'transaction_amount': 209.8, 'timestamp': '2024-09-20 21:46:57', 'location': 'London, UK', 'device_id': 'device_1'}\n",
      "Received transaction: {'transaction_id': '46230', 'user_id': '4668', 'transaction_amount': 165.81, 'timestamp': '2024-09-20 21:46:59', 'location': 'London, UK', 'device_id': 'device_55'}\n",
      "Received transaction: {'transaction_id': '76012', 'user_id': '5334', 'transaction_amount': 436.33, 'timestamp': '2024-09-20 21:47:00', 'location': 'London, UK', 'device_id': 'device_96'}\n",
      "Received transaction: {'transaction_id': '44648', 'user_id': '7126', 'transaction_amount': 205.89, 'timestamp': '2024-09-20 21:47:01', 'location': 'New York, USA', 'device_id': 'device_92'}\n",
      "Received transaction: {'transaction_id': '78432', 'user_id': '1615', 'transaction_amount': 455.16, 'timestamp': '2024-09-20 21:47:02', 'location': 'New York, USA', 'device_id': 'device_4'}\n",
      "Received transaction: {'transaction_id': '95770', 'user_id': '6850', 'transaction_amount': 458.26, 'timestamp': '2024-09-20 21:47:03', 'location': 'New York, USA', 'device_id': 'device_60'}\n",
      "Received transaction: {'transaction_id': '12725', 'user_id': '3940', 'transaction_amount': 354.09, 'timestamp': '2024-09-20 21:47:17', 'location': 'London, UK', 'device_id': 'device_27'}\n",
      "Received transaction: {'transaction_id': '11012', 'user_id': '5765', 'transaction_amount': 424.44, 'timestamp': '2024-09-20 21:47:18', 'location': 'London, UK', 'device_id': 'device_85'}\n",
      "Received transaction: {'transaction_id': '59512', 'user_id': '7742', 'transaction_amount': 77.57, 'timestamp': '2024-09-20 21:47:20', 'location': 'New York, USA', 'device_id': 'device_4'}\n",
      "Received transaction: {'transaction_id': '56951', 'user_id': '4144', 'transaction_amount': 222.75, 'timestamp': '2024-09-20 21:47:21', 'location': 'New York, USA', 'device_id': 'device_32'}\n",
      "Received transaction: {'transaction_id': '42690', 'user_id': '8864', 'transaction_amount': 22.5, 'timestamp': '2024-09-20 21:47:22', 'location': 'Berlin, Germany', 'device_id': 'device_91'}\n",
      "Received transaction: {'transaction_id': '94881', 'user_id': '8435', 'transaction_amount': 210.41, 'timestamp': '2024-09-20 21:47:23', 'location': 'New York, USA', 'device_id': 'device_8'}\n",
      "Received transaction: {'transaction_id': '34863', 'user_id': '2509', 'transaction_amount': 120.6, 'timestamp': '2024-09-20 21:47:24', 'location': 'Berlin, Germany', 'device_id': 'device_28'}\n",
      "Received transaction: {'transaction_id': '68506', 'user_id': '6593', 'transaction_amount': 261.45, 'timestamp': '2024-09-20 21:47:25', 'location': 'Berlin, Germany', 'device_id': 'device_72'}\n",
      "Received transaction: {'transaction_id': '40094', 'user_id': '2112', 'transaction_amount': 489.25, 'timestamp': '2024-09-20 21:47:26', 'location': 'New York, USA', 'device_id': 'device_52'}\n",
      "Received transaction: {'transaction_id': '70571', 'user_id': '1481', 'transaction_amount': 17.12, 'timestamp': '2024-09-20 21:47:27', 'location': 'Berlin, Germany', 'device_id': 'device_2'}\n",
      "Received transaction: {'transaction_id': '52884', 'user_id': '9971', 'transaction_amount': 193.91, 'timestamp': '2024-09-20 21:47:28', 'location': 'London, UK', 'device_id': 'device_53'}\n",
      "Received transaction: {'transaction_id': '30071', 'user_id': '3721', 'transaction_amount': 263.99, 'timestamp': '2024-09-20 21:47:29', 'location': 'London, UK', 'device_id': 'device_94'}\n",
      "Received transaction: {'transaction_id': '31672', 'user_id': '8625', 'transaction_amount': 304.93, 'timestamp': '2024-09-20 21:47:30', 'location': 'Berlin, Germany', 'device_id': 'device_16'}\n",
      "Received transaction: {'transaction_id': '82180', 'user_id': '5770', 'transaction_amount': 58.56, 'timestamp': '2024-09-20 21:47:31', 'location': 'New York, USA', 'device_id': 'device_63'}\n",
      "Received transaction: {'transaction_id': '73961', 'user_id': '9968', 'transaction_amount': 487.89, 'timestamp': '2024-09-20 21:47:32', 'location': 'New York, USA', 'device_id': 'device_94'}\n",
      "Received transaction: {'transaction_id': '55621', 'user_id': '7308', 'transaction_amount': 392.87, 'timestamp': '2024-09-20 21:47:33', 'location': 'Berlin, Germany', 'device_id': 'device_100'}\n",
      "Received transaction: {'transaction_id': '43372', 'user_id': '2681', 'transaction_amount': 497.37, 'timestamp': '2024-09-20 21:47:34', 'location': 'New York, USA', 'device_id': 'device_99'}\n",
      "Received transaction: {'transaction_id': '46165', 'user_id': '7399', 'transaction_amount': 345.36, 'timestamp': '2024-09-20 21:47:35', 'location': 'London, UK', 'device_id': 'device_78'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n",
      "Received transaction: {'transaction_id': '12345', 'user_id': '67890', 'transaction_amount': 250.75, 'timestamp': '2024-09-10 14:32:55', 'location': 'New York, USA', 'device_id': 'abc123'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m consumer \u001b[38;5;241m=\u001b[39m KafkaConsumer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransactions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m                          bootstrap_servers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                          auto_offset_reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearliest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                          value_deserializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: json\u001b[38;5;241m.\u001b[39mloads(x\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Consuming the data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[0;32m     11\u001b[0m     transaction \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived transaction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransaction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\consumer\\group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\consumer\\group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\consumer\\group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[0;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\consumer\\group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[0;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\consumer\\group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    701\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m--> 702\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    599\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    600\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[0;32m    606\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[0;32m    633\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 634\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\selectors.py:324\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\selectors.py:315\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 315\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Kafka Consumer Configuration\n",
    "consumer = KafkaConsumer('transactions',\n",
    "                         bootstrap_servers='localhost:9092',\n",
    "                         auto_offset_reset='earliest',\n",
    "                         value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "\n",
    "# Consuming the data\n",
    "for message in consumer:\n",
    "    transaction = message.value\n",
    "    print(f\"Received transaction: {transaction}\")\n",
    "    # This is where you'd later process the data with your ML model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13282e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "for message in consumer:\n",
    "    transaction = message.value\n",
    "    transaction = feature_engineering(transaction)\n",
    "    print(\"Processed transaction: {transaction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd77196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- transaction_amount: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917903ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simulate historical transaction data\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\n",
    "    'transaction_amount': [100, 200, 300, 1500, 700],\n",
    "    'location_change': [1, 0, 0, 1, 0],\n",
    "    'time_of_day': [14, 16, 19, 20, 23],\n",
    "    'is_fraud': [0, 0, 0, 1, 1]\n",
    "})\n",
    "\n",
    "# Features and labels\n",
    "X = data[['transaction_amount', 'location_change', 'time_of_day']]\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
